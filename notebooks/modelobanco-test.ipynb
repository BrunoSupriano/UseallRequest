{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Extração de API Useall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Tools and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline iniciada em 22/01/2026 08:18:19 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "BASE_URL = os.getenv(\"USEALL_BASE_URL\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"use-relatorio-token\": os.getenv(\"USEALL_TOKEN\")\n",
    "}\n",
    "\n",
    "def buscar_dados_api(identificacao, nome_arquivo, backend_filters=None, extra_params=None):\n",
    "    \"\"\"Busca dados na API UseAll e retorna um DataFrame (ou None em caso de erro/vazio)\"\"\"\n",
    "    \n",
    "    query_params = {\"Identificacao\": identificacao}\n",
    "    \n",
    "    if backend_filters:\n",
    "        query_params[\"FiltrosSqlQuery\"] = json.dumps(backend_filters, ensure_ascii=False)\n",
    "        \n",
    "    if extra_params:\n",
    "        query_params.update(extra_params)\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Iniciando extração: {nome_arquivo}...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, headers=HEADERS, params=query_params, timeout=500)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Erro 429 (Too Many Requests) em {nome_arquivo}. Aguardando 185 segundos...\")\n",
    "                time.sleep(185)\n",
    "                continue\n",
    "                \n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            return df\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Timeout atingido para {nome_arquivo}. Aguardando 185 segundos...\")\n",
    "            time.sleep(185)\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Erro irrecuperável em {nome_arquivo}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def salvar_parquet(df, nome_arquivo):\n",
    "    \"\"\"Salva o DataFrame em arquivo parquet\"\"\"\n",
    "    if df is not None and not df.empty:\n",
    "        # Garante extensão .parquet\n",
    "        if not nome_arquivo.endswith('.parquet'):\n",
    "            nome_arquivo += '.parquet'\n",
    "        \n",
    "        try:\n",
    "            df.to_parquet(nome_arquivo, index=False)\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Sucesso ao salvar: {nome_arquivo} (Linhas: {len(df)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Erro ao salvar {nome_arquivo}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Nada a salvar para {nome_arquivo} (DataFrame vazio ou None)\")\n",
    "\n",
    "def verificar_tipos_dados():\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] VERIFICAÇÃO DE TIPOS DE DADOS\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    encontrou = False\n",
    "\n",
    "    for nome, obj in globals().items():\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            encontrou = True\n",
    "            print(f\"\\nDataFrame: {nome}\")\n",
    "            if not obj.empty:\n",
    "                print(\"-\" * 30)\n",
    "                print(obj.dtypes)\n",
    "                print(\"-\" * 30)\n",
    "            else:\n",
    "                print(\"  (DataFrame vazio)\")\n",
    "\n",
    "    if not encontrou:\n",
    "        print(\"Nenhum DataFrame encontrado em memória.\")\n",
    "\n",
    "# --- Defines Auxiliares de Filtro ---\n",
    "def filtro_simples(nome, valor):\n",
    "    return {\"Nome\": nome, \"Valor\": valor, \"Operador\": None, \"Descricao\": None, \"ValorFormatado\": None}\n",
    "\n",
    "def carregar_dfs_globais(tarefas):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO CARGA EM MEMÓRIA ---\")\n",
    "\n",
    "    for t in tarefas:\n",
    "        nome = t[\"nome\"]\n",
    "        df = buscar_dados_api(\n",
    "            t[\"id\"],\n",
    "            nome,\n",
    "            t.get(\"filtros\"),\n",
    "            t.get(\"extra_params\")\n",
    "        )\n",
    "\n",
    "        if df is not None:\n",
    "            globals()[nome] = df\n",
    "        else:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "def carregar_tarefa_complexa(tarefa):\n",
    "    nome = tarefa[\"nome\"]\n",
    "\n",
    "    df = buscar_dados_api(\n",
    "        tarefa[\"id\"],\n",
    "        nome,\n",
    "        tarefa.get(\"filtros\"),\n",
    "        tarefa.get(\"extra_params\")\n",
    "    )\n",
    "\n",
    "    if df is not None:\n",
    "        globals()[nome] = df\n",
    "    else:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "\n",
    "pipeline_start = time.time()\n",
    "print(f\"--- Pipeline iniciada em {time.strftime('%d/%m/%Y %H:%M:%S')} ---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Variaveis de filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fixos = {\"pagina\": 1, \"qtderegistros\": 1}\n",
    "\n",
    "tarefas_simples = [\n",
    "    {\n",
    "        \"nome\": \"dfuseallitens\",\n",
    "        \"id\": \"m2_estoque_item\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallunidades\",\n",
    "        \"id\": \"m2_estoque_unidade\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsegmentos\",\n",
    "        \"id\": \"m2_vendas_segmento\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallcidades\",\n",
    "        \"id\": \"m2_geral_cidades\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsolcompra\",\n",
    "        \"id\": \"m2_compras_m2_compras_solicitacao_de_compras__extra\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DataFim\", \"01/01/2500\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallfiliais\",\n",
    "        \"id\": \"m2_geral_filiais\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/1900, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2500, 14:00:00\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallempresas\",\n",
    "        \"id\": \"m2_geral_empresas\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/2022, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2027, 14:00:00\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallexpedição\",\n",
    "        \"id\": \"m2_vendas_extracao_de_dados__saida_expedicao\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"data1\", \"01/01/1900\"),\n",
    "            filtro_simples(\"data2\", \"01/01/2500\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallclientesfornecedore\",\n",
    "        \"id\": \"m2_geral_clientes__fornecedores\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallalmoxarifados\",\n",
    "        \"id\": \"m2_estoque_almoxarifados\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2500\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### COMPLEXAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros_req = [\n",
    "    {\n",
    "        \"Nome\": \"IDFILIAL\",\n",
    "        \"Valor\": [\n",
    "            333,\n",
    "            339,\n",
    "            340,\n",
    "            381,\n",
    "            389,\n",
    "            336,\n",
    "            387,\n",
    "            520,\n",
    "            404,\n",
    "            558,\n",
    "            578,\n",
    "            341,\n",
    "            390,\n",
    "            345,\n",
    "            344,\n",
    "            346,\n",
    "            335,\n",
    "            334,\n",
    "            342,\n",
    "            343,\n",
    "        ],\n",
    "        \"Operador\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"DATA\",\n",
    "        \"Valor\": \"01/01/2010,01/01/2027\",\n",
    "        \"Operador\": 8,\n",
    "        \"TipoPeriodoData\": 5,\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"DATAPREVATEND\",\n",
    "        \"Valor\": \"01/01/2010,01/01/2027\",\n",
    "        \"Operador\": 8,\n",
    "        \"TipoPeriodoData\": 8,\n",
    "    },\n",
    "    {\"Nome\": \"CLASSGRUPOITEM\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"CLASSCONTACDC\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"quebra\", \"Valor\": 1},\n",
    "    {\"Nome\": \"FILTROSWHERE\", \"Valor\": \" AND IDEMPRESA = 211\"},\n",
    "]\n",
    "\n",
    "filtros_estoque = [\n",
    "    {\"Nome\": \"ADDATA\", \"Valor\": \"22/01/2026\"},\n",
    "    {\"Nome\": \"ANQUEBRA\", \"Valor\": 0},\n",
    "    {\"Nome\": \"FILTROSWHEREFORNEC\", \"Valor\": \"\"},\n",
    "    {\n",
    "        \"Nome\": \"FILTROSREGISTROSATIVO\",\n",
    "        \"Valor\": \" AND ITEM.ATIVO = 1 AND ALMOX.ATIVO = 1 AND ITEMALMOX.ATIVO = 1\",\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": \" AND EXISTS(SELECT 1 FROM USE_USUARIOS_FILIAIS UFILIAIS \"\n",
    "        \"WHERE UFILIAIS.IDEMPRESA = T.IDEMPRESA \"\n",
    "        \"AND UFILIAIS.IDFILIAL = T.IDFILIAL \"\n",
    "        \"AND UFILIAIS.IDUSUARIO = 7332) \"\n",
    "        \"AND T.IDFILIAL in (333,339, 340, 381, 389, 336, 387, 520, 404, 558, 578, 341, 390, 345, 344, 346, 335, 334, 342, 343)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"    \"Parametros\": [\n",
    "        {\n",
    "            \"Nome\": \"filter\",\n",
    "            \"Valor\": \"Filial: LOJA - ARARANGUA\\nData: 22/01/2026\\nIncluir apenas itens e almoxarifados ativos: Sim\",\n",
    "        },\n",
    "        {\"Nome\": \"quebra\", \"Valor\": 0},\n",
    "        {\"Nome\": \"ordenacao\", \"Valor\": 0},\n",
    "    ],\n",
    "    \n",
    "    \"Identificacao\": \"m2_estoque_saldo_de_estoque\",\n",
    "    \"FiltrosSql\": [\n",
    "        {\"Nome\": \"ADDATA\", \"Valor\": \"22/01/2026\"},\n",
    "        {\"Nome\": \"ANQUEBRA\", \"Valor\": 0},\n",
    "        {\"Nome\": \"FILTROSWHEREFORNEC\", \"Valor\": \"\"},\n",
    "        {\n",
    "            \"Nome\": \"FILTROSREGISTROSATIVO\",\n",
    "            \"Valor\": \" AND ITEM.ATIVO = 1 AND ALMOX.ATIVO = 1 AND ITEMALMOX.ATIVO = 1\",\n",
    "        },\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "filtros_atend = [\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": (\n",
    "            \"WHERE IDEMPRESA = 211 \"\n",
    "            \"AND IDFILIAL IN (333,339, 340, 381, 389, 336, 387, 520, 404, 558, 578, 341, 390, 345, 344, 346, 335, 334, 342, 343) \"\n",
    "            \"AND DATA_REQ BETWEEN '01/01/1900' AND '01/01/2900' \"\n",
    "            \"AND DATA_ATEND BETWEEN '01/01/1900' AND '01/01/2900'\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "params_atend = {\n",
    "    \"NomeOrganizacao\": \"SETUP SERVICOS ESPECIALIZADOS LTDA\",\n",
    "    \"Parametros\": json.dumps(\n",
    "        [{\"Nome\": \"usecellmerging\", \"Valor\": True}, {\"Nome\": \"quebra\", \"Valor\": 0}]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 1 — REQUISIÇÕES\n",
    "# ===============================\n",
    "\n",
    "tarefa_requisicoes = {\n",
    "    \"nome\": \"dfuseallrequisicoes\",\n",
    "    \"id\": \"m2_estoque_requisicao_de_materiais\",\n",
    "    \"filtros\": filtros_req,\n",
    "    \"extra_params\": None,\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 2 — ESTOQUE\n",
    "# ===============================\n",
    "\n",
    "tarefa_estoque = {\n",
    "    \"nome\": \"dfuseallestoque\",\n",
    "    \"id\": \"m2_estoque_saldo_de_estoque\",\n",
    "    \"filtros\": filtros_estoque,\n",
    "    \"extra_params\": None,\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 3 — ATENDIMENTO DE REQUISIÇÕES\n",
    "# ===============================\n",
    "\n",
    "tarefa_atendimento = {\n",
    "    \"nome\": \"dfuseallatendimentodereq\",\n",
    "    \"id\": \"m2_estoque_atendimentos_de_requisicao\",\n",
    "    \"filtros\": filtros_atend,\n",
    "    \"extra_params\": params_atend,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Criando DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Usando funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:18:20] --- INICIANDO CARGA EM MEMÓRIA ---\n",
      "[08:18:20] Iniciando extração: dfuseallitens...\n",
      "[08:18:21] Erro 429 (Too Many Requests) em dfuseallitens. Aguardando 185 segundos...\n",
      "[08:21:42] Iniciando extração: dfuseallunidades...\n",
      "[08:21:44] Iniciando extração: dfuseallsegmentos...\n",
      "[08:21:45] Iniciando extração: dfuseallcidades...\n",
      "[08:21:47] Iniciando extração: dfuseallsolcompra...\n",
      "[08:21:49] Iniciando extração: dfuseallfiliais...\n",
      "[08:21:50] Iniciando extração: dfuseallempresas...\n",
      "[08:21:53] Iniciando extração: dfuseallexpedição...\n",
      "[08:22:24] Iniciando extração: dfuseallclientesfornecedore...\n",
      "[08:22:41] Iniciando extração: dfuseallalmoxarifados...\n"
     ]
    }
   ],
   "source": [
    "carregar_dfs_globais(tarefas_simples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:51:38] Iniciando extração: dfuseallestoque...\n",
      "[13:51:40] Erro 429 (Too Many Requests) em dfuseallestoque. Aguardando 185 segundos...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_estoque)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:23:45] Iniciando extração: dfuseallatendimentodereq...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_atendimento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:41] Iniciando extração: dfuseallrequisicoes...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_requisicoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Api Custos - Particularidade de loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 08:34:41 | INFO | INÍCIO DO PIPELINE\n",
      "INFO:useall_pipeline:INÍCIO DO PIPELINE\n",
      "2026-01-22 08:34:41 | INFO | Raw do dia não encontrado. Iniciando download por grupos.\n",
      "INFO:useall_pipeline:Raw do dia não encontrado. Iniciando download por grupos.\n",
      "2026-01-22 08:34:41 | INFO | INÍCIO GRUPO 1/6: ctfm\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 1/6: ctfm\n",
      "2026-01-22 08:34:42 | INFO | Requisição API | Grupo ctfm\n",
      "INFO:useall_pipeline:Requisição API | Grupo ctfm\n",
      "2026-01-22 08:35:16 | INFO | Resposta API OK | Grupo ctfm | duração: 34.66s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo ctfm | duração: 34.66s\n",
      "2026-01-22 08:35:23 | INFO | Arquivo salvo | data/raw/ctfm.parquet | registros: 21723 | duração: 1.88s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/ctfm.parquet | registros: 21723 | duração: 1.88s\n",
      "2026-01-22 08:35:23 | INFO | FIM GRUPO: ctfm | duração: 41.25s\n",
      "INFO:useall_pipeline:FIM GRUPO: ctfm | duração: 41.25s\n",
      "2026-01-22 08:35:23 | INFO | Cooldown 185s antes do próximo grupo\n",
      "INFO:useall_pipeline:Cooldown 185s antes do próximo grupo\n",
      "2026-01-22 08:38:28 | INFO | INÍCIO GRUPO 2/6: lojas\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 2/6: lojas\n",
      "2026-01-22 08:38:28 | INFO | Requisição API | Grupo lojas\n",
      "INFO:useall_pipeline:Requisição API | Grupo lojas\n",
      "2026-01-22 08:39:03 | INFO | Resposta API OK | Grupo lojas | duração: 35.22s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo lojas | duração: 35.22s\n",
      "2026-01-22 08:39:08 | INFO | Arquivo salvo | data/raw/lojas.parquet | registros: 26646 | duração: 1.30s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/lojas.parquet | registros: 26646 | duração: 1.30s\n",
      "2026-01-22 08:39:08 | INFO | FIM GRUPO: lojas | duração: 40.24s\n",
      "INFO:useall_pipeline:FIM GRUPO: lojas | duração: 40.24s\n",
      "2026-01-22 08:39:08 | INFO | Cooldown 185s antes do próximo grupo\n",
      "INFO:useall_pipeline:Cooldown 185s antes do próximo grupo\n",
      "2026-01-22 08:42:13 | INFO | INÍCIO GRUPO 3/6: vm\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 3/6: vm\n",
      "2026-01-22 08:42:13 | INFO | Requisição API | Grupo vm\n",
      "INFO:useall_pipeline:Requisição API | Grupo vm\n",
      "2026-01-22 08:42:41 | INFO | Resposta API OK | Grupo vm | duração: 28.15s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo vm | duração: 28.15s\n",
      "2026-01-22 08:42:42 | INFO | Arquivo salvo | data/raw/vm.parquet | registros: 22769 | duração: 0.27s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/vm.parquet | registros: 22769 | duração: 0.27s\n",
      "2026-01-22 08:42:42 | INFO | FIM GRUPO: vm | duração: 29.19s\n",
      "INFO:useall_pipeline:FIM GRUPO: vm | duração: 29.19s\n",
      "2026-01-22 08:42:42 | INFO | Cooldown 185s antes do próximo grupo\n",
      "INFO:useall_pipeline:Cooldown 185s antes do próximo grupo\n",
      "2026-01-22 08:45:47 | INFO | INÍCIO GRUPO 4/6: setup_automacao\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 4/6: setup_automacao\n",
      "2026-01-22 08:45:47 | INFO | Requisição API | Grupo setup_automacao\n",
      "INFO:useall_pipeline:Requisição API | Grupo setup_automacao\n",
      "2026-01-22 08:46:35 | INFO | Resposta API OK | Grupo setup_automacao | duração: 47.78s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo setup_automacao | duração: 47.78s\n",
      "2026-01-22 08:46:41 | INFO | Arquivo salvo | data/raw/setup_automacao.parquet | registros: 97926 | duração: 1.65s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/setup_automacao.parquet | registros: 97926 | duração: 1.65s\n",
      "2026-01-22 08:46:41 | INFO | FIM GRUPO: setup_automacao | duração: 53.61s\n",
      "INFO:useall_pipeline:FIM GRUPO: setup_automacao | duração: 53.61s\n",
      "2026-01-22 08:46:41 | INFO | Cooldown 185s antes do próximo grupo\n",
      "INFO:useall_pipeline:Cooldown 185s antes do próximo grupo\n",
      "2026-01-22 08:49:46 | INFO | INÍCIO GRUPO 5/6: servicos\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 5/6: servicos\n",
      "2026-01-22 08:49:46 | INFO | Requisição API | Grupo servicos\n",
      "INFO:useall_pipeline:Requisição API | Grupo servicos\n",
      "2026-01-22 08:50:22 | INFO | Resposta API OK | Grupo servicos | duração: 36.46s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo servicos | duração: 36.46s\n",
      "2026-01-22 08:50:23 | INFO | Arquivo salvo | data/raw/servicos.parquet | registros: 12075 | duração: 0.08s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/servicos.parquet | registros: 12075 | duração: 0.08s\n",
      "2026-01-22 08:50:23 | INFO | FIM GRUPO: servicos | duração: 37.15s\n",
      "INFO:useall_pipeline:FIM GRUPO: servicos | duração: 37.15s\n",
      "2026-01-22 08:50:23 | INFO | Cooldown 185s antes do próximo grupo\n",
      "INFO:useall_pipeline:Cooldown 185s antes do próximo grupo\n",
      "2026-01-22 08:53:28 | INFO | INÍCIO GRUPO 6/6: setup\n",
      "INFO:useall_pipeline:INÍCIO GRUPO 6/6: setup\n",
      "2026-01-22 08:53:28 | INFO | Requisição API | Grupo setup\n",
      "INFO:useall_pipeline:Requisição API | Grupo setup\n",
      "2026-01-22 08:54:31 | INFO | Resposta API OK | Grupo setup | duração: 62.52s\n",
      "INFO:useall_pipeline:Resposta API OK | Grupo setup | duração: 62.52s\n",
      "2026-01-22 08:54:41 | INFO | Arquivo salvo | data/raw/setup.parquet | registros: 148890 | duração: 1.48s\n",
      "INFO:useall_pipeline:Arquivo salvo | data/raw/setup.parquet | registros: 148890 | duração: 1.48s\n",
      "2026-01-22 08:54:41 | INFO | FIM GRUPO: setup | duração: 72.70s\n",
      "INFO:useall_pipeline:FIM GRUPO: setup | duração: 72.70s\n",
      "2026-01-22 08:54:41 | INFO | FIM DO PIPELINE | duração: 1199.59s\n",
      "INFO:useall_pipeline:FIM DO PIPELINE | duração: 1199.59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1769082881.5400157"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# IMPORTS E ENV\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "BASE_URL = os.getenv(\"USEALL_BASE_URL\")\n",
    "TOKEN = os.getenv(\"USEALL_TOKEN\")\n",
    "\n",
    "IDENTIFICACAO = \"m2_estoque_custos\"\n",
    "DATA_REF = datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%d/%m/%Y\")\n",
    "ESPERA = 185\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"use-relatorio-token\": TOKEN\n",
    "}\n",
    "\n",
    "# %%\n",
    "# LOGGING (Jupyter-safe)\n",
    "logger = logging.getLogger(\"useall_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# %%\n",
    "# PATHS\n",
    "RAW_DIR = \"data/raw\"\n",
    "RAW_FINAL = \"data/staging_custos_raw.parquet\"\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# %%\n",
    "# FUNÇÕES AUXILIARES\n",
    "def ja_baixado_hoje(path: str) -> bool:\n",
    "    if not os.path.exists(path):\n",
    "        return False\n",
    "    mod_time = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "    return mod_time.date() == datetime.today().date()\n",
    "\n",
    "\n",
    "def log_etapa(msg: str, inicio: float | None = None) -> float:\n",
    "    agora = time.time()\n",
    "    if inicio:\n",
    "        logger.info(f\"{msg} | duração: {agora - inicio:.2f}s\")\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    return agora\n",
    "\n",
    "# %%\n",
    "# GRUPOS\n",
    "grupos = {\n",
    "    \"ctfm\": [342, 343],\n",
    "    \"lojas\": [335, 334],\n",
    "    \"vm\": [345, 344, 346],\n",
    "    \"setup_automacao\": [333],\n",
    "    \"servicos\": [339, 340, 381, 389],\n",
    "    \"setup\": [336, 387, 520, 404, 558, 578, 341, 390],\n",
    "}\n",
    "\n",
    "\n",
    "# %%\n",
    "# PIPELINE\n",
    "pipeline_start = log_etapa(\"INÍCIO DO PIPELINE\")\n",
    "\n",
    "if ja_baixado_hoje(RAW_FINAL):\n",
    "    logger.info(\"Raw consolidado já existe e é de hoje. Pipeline encerrado.\")\n",
    "else:\n",
    "    logger.info(\"Raw do dia não encontrado. Iniciando download por grupos.\")\n",
    "\n",
    "    nomes_grupos = list(grupos.items())\n",
    "    total_grupos = len(nomes_grupos)\n",
    "\n",
    "    for idx, (nome, ids) in enumerate(nomes_grupos, start=1):\n",
    "        ultimo_grupo = idx == total_grupos\n",
    "        grupo_start = log_etapa(f\"INÍCIO GRUPO {idx}/{total_grupos}: {nome}\")\n",
    "\n",
    "        raw_path = f\"{RAW_DIR}/{nome}.parquet\"\n",
    "\n",
    "        if ja_baixado_hoje(raw_path):\n",
    "            logger.info(f\"{nome} já existe e é de hoje. Pulando grupo.\")\n",
    "            continue\n",
    "\n",
    "        filtros = [\n",
    "            {\"Nome\": \"idfilial\", \"Valor\": ids, \"Operador\": 1},\n",
    "            {\"Nome\": \"FILTROSREGISTROSATIVO\", \"Valor\": \"\"},\n",
    "            {\"Nome\": \"filtroswhere\", \"Valor\": f\" AND IDFILIAL IN ({','.join(map(str, ids))})\"},\n",
    "            {\"Nome\": \"data\", \"Valor\": DATA_REF}\n",
    "        ]\n",
    "\n",
    "        params = {\n",
    "            \"Identificacao\": IDENTIFICACAO,\n",
    "            \"FiltrosSqlQuery\": json.dumps(filtros, ensure_ascii=False)\n",
    "        }\n",
    "\n",
    "        request_start = log_etapa(f\"Requisição API | Grupo {nome}\")\n",
    "\n",
    "        while True:\n",
    "            r = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=180)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                log_etapa(f\"Resposta API OK | Grupo {nome}\", request_start)\n",
    "\n",
    "                payload = r.json()\n",
    "                registros = payload.get(\"data\") if isinstance(payload, dict) else payload\n",
    "\n",
    "                if registros:\n",
    "                    for row in registros:\n",
    "                        row[\"_grupo_origem\"] = nome\n",
    "\n",
    "                    df = pd.DataFrame(registros)\n",
    "\n",
    "                    save_start = time.time()\n",
    "                    df.to_parquet(raw_path, engine=\"pyarrow\", index=False)\n",
    "                    log_etapa(\n",
    "                        f\"Arquivo salvo | {raw_path} | registros: {len(df)}\",\n",
    "                        save_start\n",
    "                    )\n",
    "                else:\n",
    "                    logger.warning(f\"Grupo {nome} retornou 0 registros\")\n",
    "\n",
    "                break\n",
    "\n",
    "            if r.status_code == 429:\n",
    "                logger.warning(f\"429 Rate limit | {nome} | aguardando {ESPERA}s\")\n",
    "                time.sleep(ESPERA)\n",
    "                continue\n",
    "\n",
    "            if r.status_code == 400:\n",
    "                logger.error(f\"400 Payload pesado | Grupo {nome}\")\n",
    "                break\n",
    "\n",
    "            r.raise_for_status()\n",
    "\n",
    "        log_etapa(f\"FIM GRUPO: {nome}\", grupo_start)\n",
    "\n",
    "        # espera somente se NÃO for o último grupo\n",
    "        if not ultimo_grupo:\n",
    "            logger.info(f\"Cooldown {ESPERA}s antes do próximo grupo\")\n",
    "            time.sleep(ESPERA)\n",
    "\n",
    "log_etapa(\"FIM DO PIPELINE\", pipeline_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_registros = []\n",
    "\n",
    "for arquivo in os.listdir(RAW_DIR):\n",
    "    if arquivo.endswith(\".parquet\"):\n",
    "        path = os.path.join(RAW_DIR, arquivo)\n",
    "\n",
    "        df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "        # origem = nome do arquivo sem extensão\n",
    "        grupo_origem = os.path.splitext(arquivo)[0]\n",
    "        df[\"_grupo_origem\"] = grupo_origem\n",
    "\n",
    "        todos_registros.append(df)\n",
    "        logging.info(f\"Lido {arquivo} ({len(df)} registros)\")\n",
    "\n",
    "if todos_registros:\n",
    "    df_final = pd.concat(todos_registros, ignore_index=True)\n",
    "    df_final.to_parquet(RAW_FINAL, engine=\"pyarrow\", index=False)\n",
    "    logging.info(\n",
    "        f\"Arquivo consolidado criado: {RAW_FINAL} ({len(df_final)} registros)\"\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"Nenhum arquivo Parquet encontrado em RAW_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Verificando Tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:55:01] --- INICIANDO VERIFICAÇÃO DE TIPOS ---\n",
      "\n",
      "========================================\n",
      "[08:55:01] VERIFICAÇÃO DE TIPOS DE DADOS\n",
      "========================================\n",
      "\n",
      "DataFrame: __\n",
      "------------------------------\n",
      "IDFILIAL                int64\n",
      "ULTIMOCUSTO           float64\n",
      "CUSTOMEDIO            float64\n",
      "DATACUSTOMEDIO         object\n",
      "DATACUSTO              object\n",
      "CODIGOCONTACTB          int64\n",
      "CODIGOCONTA             int64\n",
      "DESCRICAOCONTACTB      object\n",
      "DATAALMOX              object\n",
      "CODIGOGRUPOITEM       float64\n",
      "DESCRICAOGRUPOITEM     object\n",
      "CODIGOITEM              int64\n",
      "DESCRICAO              object\n",
      "CODIGOALMOX             int64\n",
      "CUSTOMEDIOALMOX       float64\n",
      "IDENTIFICACAO          object\n",
      "SIGLA                  object\n",
      "CODIGOTIPOITEM          int64\n",
      "CODIGOMARCAITEM       float64\n",
      "CLASS_GRUPOITEM        object\n",
      "SALDO                 float64\n",
      "IDALMOXTENANT         float64\n",
      "CODIGOFORNECEDOR      float64\n",
      "_grupo_origem          object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: df\n",
      "------------------------------\n",
      "IDFILIAL                int64\n",
      "ULTIMOCUSTO           float64\n",
      "CUSTOMEDIO            float64\n",
      "DATACUSTOMEDIO         object\n",
      "DATACUSTO              object\n",
      "CODIGOCONTACTB          int64\n",
      "CODIGOCONTA             int64\n",
      "DESCRICAOCONTACTB      object\n",
      "DATAALMOX              object\n",
      "CODIGOGRUPOITEM       float64\n",
      "DESCRICAOGRUPOITEM     object\n",
      "CODIGOITEM              int64\n",
      "DESCRICAO              object\n",
      "CODIGOALMOX             int64\n",
      "CUSTOMEDIOALMOX       float64\n",
      "IDENTIFICACAO          object\n",
      "SIGLA                  object\n",
      "CODIGOTIPOITEM          int64\n",
      "CODIGOMARCAITEM       float64\n",
      "CLASS_GRUPOITEM        object\n",
      "SALDO                 float64\n",
      "IDALMOXTENANT         float64\n",
      "CODIGOFORNECEDOR      float64\n",
      "_grupo_origem          object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallitens\n",
      "------------------------------\n",
      "IDITEM                 int64\n",
      "DATAHORAALTERACAO     object\n",
      "IDENTIFICACAO         object\n",
      "DESCRICAO             object\n",
      "IDCLASSFISCAL        float64\n",
      "IDUN                 float64\n",
      "IDUNVENDA            float64\n",
      "QUANTEMBVENDA        float64\n",
      "PESOBRUTO            float64\n",
      "PESOLIQ              float64\n",
      "OBS                   object\n",
      "ATIVO                  int64\n",
      "COMPLEMENTO           object\n",
      "IDGRUPOITEM          float64\n",
      "IDTIPOITEM             int64\n",
      "QUANTMINVENDA        float64\n",
      "CODBARRA              object\n",
      "LIBERADO              object\n",
      "FORMATOCODBAR        float64\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallunidades\n",
      "------------------------------\n",
      "IDUN                  int64\n",
      "IDUNIDADETENANT       int64\n",
      "DESCRICAO            object\n",
      "SIGLA                object\n",
      "SIGLAFCI             object\n",
      "DATAHORAALTERACAO    object\n",
      "STATUS               object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallsegmentos\n",
      "  (DataFrame vazio)\n",
      "\n",
      "DataFrame: dfuseallcidades\n",
      "------------------------------\n",
      "IDCIDADE               int64\n",
      "DATAHORAALTERACAO     object\n",
      "NOME                  object\n",
      "MUNICIPIO_IBGE         int64\n",
      "IDNFSE               float64\n",
      "CODIGORFB            float64\n",
      "DATAHORACAD           object\n",
      "CODIBGE                int64\n",
      "SIGLA                 object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallsolcompra\n",
      "------------------------------\n",
      "IDSOLICCOMPRA                int64\n",
      "IDEMPRESA                    int64\n",
      "IDFILIAL                     int64\n",
      "NUMERO                       int64\n",
      "IDSOLICITANTE                int64\n",
      "OBS                         object\n",
      "STATUS                      object\n",
      "PRIMEIRADATANECESSIDADE     object\n",
      "FINALIDADE                  object\n",
      "IDSOLICCOMPRAITEM            int64\n",
      "IDENTIFICACAO               object\n",
      "IDUN                         int64\n",
      "QUANT                      float64\n",
      "DATANECESSIDADE             object\n",
      "COMPLEMENTO                 object\n",
      "SALDO                      float64\n",
      "URGENTE                      int64\n",
      "URGENTE_DESC                object\n",
      "IDCONTACDC                 float64\n",
      "CLASSIFICACAO               object\n",
      "DESCRICAO                   object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallfiliais\n",
      "------------------------------\n",
      "IDFILIAL        int64\n",
      "DATAHORAALT    object\n",
      "IDEMPRESA       int64\n",
      "MATRIZ          int64\n",
      "APELIDO        object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallempresas\n",
      "------------------------------\n",
      "IDEMPRESA        int64\n",
      "DATAHORAALT     object\n",
      "CNPJCPF         object\n",
      "RAZAOSOCIAL     object\n",
      "NOMEFANTASIA    object\n",
      "IE              object\n",
      "IM              object\n",
      "ENDERECO        object\n",
      "BAIRRO          object\n",
      "CEP             object\n",
      "FONE            object\n",
      "FAX             object\n",
      "EMAIL           object\n",
      "IDCIDADE         int64\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallexpedição\n",
      "------------------------------\n",
      "CODIGOFAT                       int64\n",
      "DATAHORACAD                    object\n",
      "DATAENTREGA                    object\n",
      "CONFERENCIA                    object\n",
      "ORIGEMCONFERENCIA             float64\n",
      "DESCRICAOORIGEMCONFERENCIA     object\n",
      "ORIGEM                         object\n",
      "CODIGOOIRIGEM                   int64\n",
      "NUMERODOC                       int64\n",
      "NUMEROPEDIDOCLIENTE            object\n",
      "CODIGOCLIENTE                 float64\n",
      "RAZAOCLIENTE                   object\n",
      "ENDERECO                       object\n",
      "CEP                            object\n",
      "CIDADE                         object\n",
      "UF                             object\n",
      "ENDERECOENTREGA                object\n",
      "CEPENTREGA                     object\n",
      "CIDADEENTREGA                  object\n",
      "UFENTREGA                      object\n",
      "CODIGOTRANSP                  float64\n",
      "RAZAOTRANSP                    object\n",
      "RAWTOGUID(GUIDLOTE)            object\n",
      "CARGA                          object\n",
      "CODIGOUSUARIO                 float64\n",
      "NOMEUSUARIO                    object\n",
      "CODIGOGRUPOEMPRESARIAL        float64\n",
      "DESCRICAOGRUPOEMPRESARIAL      object\n",
      "NUMEROITEMPEDIDOCLIENTE        object\n",
      "SEQ                             int64\n",
      "IDENTIFICACAO                  object\n",
      "DESCRICAO                      object\n",
      "UNIDADE                        object\n",
      "QUANTSELECIONADA              float64\n",
      "QUANTCONFERIDA                float64\n",
      "SALDO                         float64\n",
      "ORDEM                           int64\n",
      "CODIGOFILIAL                    int64\n",
      "CODIGOEMPRESA                   int64\n",
      "PLACA                          object\n",
      "ORDEMENTREGA                   object\n",
      "DATAPREVCARREG                 object\n",
      "STATUSCONF                      int64\n",
      "CODIGOTRANSPTENANT            float64\n",
      "DATAHORAPREPARACAO             object\n",
      "IDPEDITEM                       int64\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallclientesfornecedore\n",
      "------------------------------\n",
      "IDCLIFOREMP                 int64\n",
      "DATAHORAALTERACAO          object\n",
      "IDCLIFOREMPTENANT           int64\n",
      "DATAHORACAD                object\n",
      "NOMEFANTASIA               object\n",
      "RAZAOSOCIAL                object\n",
      "CNPJCPF                    object\n",
      "FONE                       object\n",
      "EMAIL                      object\n",
      "IE                         object\n",
      "RG                         object\n",
      "OBS                        object\n",
      "ISUFRAMA                   object\n",
      "ATIVO                       int64\n",
      "CONTRIBICMS                 int64\n",
      "IDTRANSP                  float64\n",
      "CEP                        object\n",
      "IDCIDADE                    int64\n",
      "ENDERECO                   object\n",
      "NUMENDERECO                object\n",
      "BAIRRO                     object\n",
      "COMPLENDERECO              object\n",
      "CEPENT                     object\n",
      "IDCIDADEENT                 int64\n",
      "ENDERECOENT                object\n",
      "NUMENDERECOENT             object\n",
      "BAIRROENT                  object\n",
      "COMPLENDERECOENT           object\n",
      "CEPCOB                     object\n",
      "IDCIDADECOB                 int64\n",
      "ENDERECOCOB                object\n",
      "NUMENDERECOCOB             object\n",
      "BAIRROCOB                  object\n",
      "COMPLENDERECOCOB           object\n",
      "LATITUDE                  float64\n",
      "LONGITUDE                 float64\n",
      "DATALIMCREDITO             object\n",
      "VALORLIMCREDITO           float64\n",
      "IDRISCOCREDITO            float64\n",
      "IDTABPRECOVENDA           float64\n",
      "IDCONDPAGTO               float64\n",
      "IDFORMAPAGTO              float64\n",
      "IDVENDEDOR                float64\n",
      "IDSUPERVISOR              float64\n",
      "SEGMENTO                   object\n",
      "IDPAIS                      int64\n",
      "CLIENTE                    object\n",
      "EMPREGADO                  object\n",
      "FORNECEDOR                 object\n",
      "VENDEDOR_REPRESENTANTE     object\n",
      "TRANSPORTADOR              object\n",
      "TIPOPESSOA                 object\n",
      "REGIMETRIB                 object\n",
      "TIPOFRETE                  object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallalmoxarifados\n",
      "------------------------------\n",
      "IDALMOX               int64\n",
      "DATAHORAALT          object\n",
      "IDALMOXTENANT         int64\n",
      "IDEMPRESA             int64\n",
      "IDFILIAL              int64\n",
      "DESCRICAO            object\n",
      "DATAIMPLANTACAO      object\n",
      "IDCONTACTBESTOQUE     int64\n",
      "SIGLA                object\n",
      "TIPO                 object\n",
      "STATUS               object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallestoque\n",
      "------------------------------\n",
      "IDSECAO            float64\n",
      "IDFILIAL             int64\n",
      "IDEMPRESA            int64\n",
      "IDALMOX              int64\n",
      "TIPOALMOX            int64\n",
      "DESC_ALMOX          object\n",
      "IDUN                 int64\n",
      "IDITEM               int64\n",
      "IDENTIFICACAO       object\n",
      "DESC_ITEM           object\n",
      "IDMARCA            float64\n",
      "IDTIPOITEM           int64\n",
      "IDGRUPOITEM          int64\n",
      "CLASSIFICACAO       object\n",
      "DESC_GRUPO          object\n",
      "DESC_MARCA          object\n",
      "SALDOFISICO        float64\n",
      "QUANTRESERVADA     float64\n",
      "SALDODISPONIVEL    float64\n",
      "ESTOQUEMIN         float64\n",
      "ESTOQUEMAX         float64\n",
      "DESC_TIPOITEM       object\n",
      "SIGLA               object\n",
      "POSICAO            float64\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallatendimentodereq\n",
      "------------------------------\n",
      "DESC_FILIAL              object\n",
      "ENDERECO_FILIAL          object\n",
      "CEP_FILIAL               object\n",
      "FAX_FILIAL               object\n",
      "FONE_FILIAL              object\n",
      "CPF_CNPJ_FILIAL          object\n",
      "INSC_ESTADUAL_FILIAL     object\n",
      "CIDADE_FILIAL            object\n",
      "UF_FILIAL                object\n",
      "DATA_ATEND               object\n",
      "IDEMPREGADO_ATEND         int64\n",
      "QUANT_ATEND             float64\n",
      "IDEMPRESA                 int64\n",
      "IDFILIAL                  int64\n",
      "IDREQMAT                  int64\n",
      "NUMEROOS                 object\n",
      "NUMEROPROCSERV          float64\n",
      "NUMEROOP                 object\n",
      "DATA_REQ                 object\n",
      "IDALMOX                   int64\n",
      "TIPO                      int64\n",
      "DESC_ALMOX               object\n",
      "IDREQUISITANTE          float64\n",
      "REQUISITANTE             object\n",
      "SIGLA_UN                 object\n",
      "IDITEM                    int64\n",
      "IDENTIFICACAO            object\n",
      "DESCRICAO                object\n",
      "IDGRUPOITEM               int64\n",
      "IDATENDREQMATITEM         int64\n",
      "IDGRUPOPAI              float64\n",
      "DESC_GRUPO               object\n",
      "EMPREGADO_ATEND          object\n",
      "IDCONTACDC              float64\n",
      "IDCONTACAR               object\n",
      "DESCRICAOCONTACDC        object\n",
      "CLASSIFICACAOCDC         object\n",
      "DESCRICAOCONTACAR        object\n",
      "CLASSIFICACAOCAR         object\n",
      "ESTOQUEFECHADO          float64\n",
      "CUSTOMEDIO              float64\n",
      "QUANTLOTE               float64\n",
      "QUANT_ATEND_LOTE        float64\n",
      "NUMLOTE                  object\n",
      "IDREQMATTENANT          float64\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: df_final\n",
      "------------------------------\n",
      "IDFILIAL                int64\n",
      "ULTIMOCUSTO           float64\n",
      "CUSTOMEDIO            float64\n",
      "DATACUSTOMEDIO         object\n",
      "DATACUSTO              object\n",
      "CODIGOCONTACTB          int64\n",
      "CODIGOCONTA             int64\n",
      "DESCRICAOCONTACTB      object\n",
      "DATAALMOX              object\n",
      "CODIGOGRUPOITEM       float64\n",
      "DESCRICAOGRUPOITEM     object\n",
      "CODIGOITEM              int64\n",
      "DESCRICAO              object\n",
      "CODIGOALMOX             int64\n",
      "CUSTOMEDIOALMOX       float64\n",
      "IDENTIFICACAO          object\n",
      "SIGLA                  object\n",
      "CODIGOTIPOITEM          int64\n",
      "CODIGOMARCAITEM       float64\n",
      "CLASS_GRUPOITEM        object\n",
      "SALDO                 float64\n",
      "IDALMOXTENANT         float64\n",
      "CODIGOFORNECEDOR      float64\n",
      "_grupo_origem          object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: teste\n",
      "------------------------------\n",
      "IDFILIAL                int64\n",
      "ULTIMOCUSTO           float64\n",
      "CUSTOMEDIO            float64\n",
      "DATACUSTOMEDIO         object\n",
      "DATACUSTO              object\n",
      "CODIGOCONTACTB          int64\n",
      "CODIGOCONTA             int64\n",
      "DESCRICAOCONTACTB      object\n",
      "DATAALMOX              object\n",
      "CODIGOGRUPOITEM       float64\n",
      "DESCRICAOGRUPOITEM     object\n",
      "CODIGOITEM              int64\n",
      "DESCRICAO              object\n",
      "CODIGOALMOX             int64\n",
      "CUSTOMEDIOALMOX       float64\n",
      "IDENTIFICACAO          object\n",
      "SIGLA                  object\n",
      "CODIGOTIPOITEM          int64\n",
      "CODIGOMARCAITEM       float64\n",
      "CLASS_GRUPOITEM        object\n",
      "SALDO                 float64\n",
      "IDALMOXTENANT         float64\n",
      "CODIGOFORNECEDOR      float64\n",
      "_grupo_origem          object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: _47\n",
      "------------------------------\n",
      "IDFILIAL                int64\n",
      "ULTIMOCUSTO           float64\n",
      "CUSTOMEDIO            float64\n",
      "DATACUSTOMEDIO         object\n",
      "DATACUSTO              object\n",
      "CODIGOCONTACTB          int64\n",
      "CODIGOCONTA             int64\n",
      "DESCRICAOCONTACTB      object\n",
      "DATAALMOX              object\n",
      "CODIGOGRUPOITEM       float64\n",
      "DESCRICAOGRUPOITEM     object\n",
      "CODIGOITEM              int64\n",
      "DESCRICAO              object\n",
      "CODIGOALMOX             int64\n",
      "CUSTOMEDIOALMOX       float64\n",
      "IDENTIFICACAO          object\n",
      "SIGLA                  object\n",
      "CODIGOTIPOITEM          int64\n",
      "CODIGOMARCAITEM       float64\n",
      "CLASS_GRUPOITEM        object\n",
      "SALDO                 float64\n",
      "IDALMOXTENANT         float64\n",
      "CODIGOFORNECEDOR      float64\n",
      "_grupo_origem          object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "DataFrame: dfuseallrequisicoes\n",
      "------------------------------\n",
      "DESC_FILIAL                object\n",
      "ENDERECO_FILIAL            object\n",
      "CEP_FILIAL                 object\n",
      "FAX_FILIAL                 object\n",
      "FONE_FILIAL                object\n",
      "CPF_CNPJ_FILIAL            object\n",
      "INSC_ESTADUAL_FILIAL       object\n",
      "CIDADE_FILIAL              object\n",
      "UF_FILIAL                  object\n",
      "IDEMPRESA                   int64\n",
      "IDFILIAL                    int64\n",
      "IDREQMAT                    int64\n",
      "IDREQMATTENANT              int64\n",
      "DATA                       object\n",
      "DATAPREVATEND              object\n",
      "IDALMOX                     int64\n",
      "STATUS                      int64\n",
      "TIPO                        int64\n",
      "DESC_ALMOX                 object\n",
      "SIGLA_ALMOX                object\n",
      "IDTENANT                    int64\n",
      "IDREQMATITEM                int64\n",
      "QUANT                     float64\n",
      "QUANTCANCEL               float64\n",
      "QUANTATEND                float64\n",
      "QUANTSUBST                float64\n",
      "SALDO                     float64\n",
      "IDREQUISITANTE            float64\n",
      "REQUISITANTE               object\n",
      "SIGLA_UN                   object\n",
      "IDITEM                      int64\n",
      "IDENTIFICACAO              object\n",
      "DESCRICAO                  object\n",
      "IDGRUPOITEM                 int64\n",
      "IDCONTACDC                float64\n",
      "CLASSIFICACAOGRUPOITEM     object\n",
      "DESC_GRUPO                 object\n",
      "DESCRICAOCONTACDC          object\n",
      "CLASSIFICACAOCONTACDC      object\n",
      "dtype: object\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Verificação de Tipos ---\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO VERIFICAÇÃO DE TIPOS ---\")\n",
    "verificar_tipos_dados()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Configurações Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando .env\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "user = quote(os.getenv(\"PG_USER\"))\n",
    "password = quote(os.getenv(\"PG_PASSWORD\"))\n",
    "host = os.getenv(\"PG_HOST\")\n",
    "port = os.getenv(\"PG_PORT\")\n",
    "dbname = os.getenv(\"PG_DBNAME\")\n",
    "\n",
    "SCHEMA = os.getenv(\"DB_SCHEMA\")\n",
    "\n",
    "DB_URL = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "# garante schema\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\"))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Staging - Bronze - Dados Brutos tipos indefinidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela useall.staging_custos substituída via COPY.\n"
     ]
    }
   ],
   "source": [
    "# custos - particularidade staging\n",
    "import io\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "parquet_file = \"data/staging_custos_raw.parquet\"\n",
    "table_name = \"staging_custos\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "if not df.empty:\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(\n",
    "        csv_buffer,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep=\",\",\n",
    "        quotechar='\"',\n",
    "        quoting=1,        # csv.QUOTE_ALL\n",
    "        escapechar=\"\\\\\"\n",
    "    )\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    cols_with_types = \", \".join([f'\"{col}\" TEXT' for col in df.columns])\n",
    "\n",
    "    # garante tabela + limpa dados (FAST)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {SCHEMA}.{table_name}\"))\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{table_name} (\n",
    "                {cols_with_types}\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "    # COPY ultra-rápido\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "    cursor.copy_expert(\n",
    "        f\"\"\"\n",
    "        COPY {SCHEMA}.{table_name}\n",
    "        FROM STDIN\n",
    "        WITH (\n",
    "            FORMAT CSV,\n",
    "            QUOTE '\"',\n",
    "            ESCAPE '\\\\'\n",
    "        )\n",
    "        \"\"\",\n",
    "        csv_buffer\n",
    "    )\n",
    "    raw_conn.commit()\n",
    "    cursor.close()\n",
    "    raw_conn.close()\n",
    "\n",
    "    print(f\"Tabela {SCHEMA}.{table_name} substituída via COPY.\")\n",
    "else:\n",
    "    print(\"Parquet vazio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_staging = [\n",
    "    # simples iniciais\n",
    "    \"dfuseallitens\",\n",
    "    \"dfuseallunidades\",\n",
    "    \"dfuseallsegmentos\",\n",
    "    \"dfuseallcidades\",\n",
    "\n",
    "    # complexas no meio\n",
    "    \"dfuseallrequisicoes\",\n",
    "    \"dfuseallestoque\",\n",
    "    \"dfuseallatendimentodereq\",\n",
    "\n",
    "    # simples finais\n",
    "    \"dfuseallsolcompra\",\n",
    "    \"dfuseallfiliais\",\n",
    "    \"dfuseallempresas\",\n",
    "    \"dfuseallexpedição\",\n",
    "    \"dfuseallclientesfornecedore\",\n",
    "    \"dfuseallalmoxarifados\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:03:26] INICIANDO CARGA STAGING (COPY FROM)\n",
      "[14:03:26] Preparando tabela useall.staging_itens | Linhas: 16012\n",
      "[14:03:27] Iniciando COPY para useall.staging_itens\n",
      "[14:03:28] [OK] Tabela useall.staging_itens carregada com sucesso\n",
      "[14:03:28] Preparando tabela useall.staging_unidades | Linhas: 60\n",
      "[14:03:29] Iniciando COPY para useall.staging_unidades\n",
      "[14:03:29] [OK] Tabela useall.staging_unidades carregada com sucesso\n",
      "[14:03:29] Preparando tabela useall.staging_cidades | Linhas: 6148\n",
      "[14:03:29] Iniciando COPY para useall.staging_cidades\n",
      "[14:03:29] [OK] Tabela useall.staging_cidades carregada com sucesso\n",
      "[14:03:29] Preparando tabela useall.staging_requisicoes | Linhas: 684516\n",
      "[14:03:29] Iniciando COPY para useall.staging_requisicoes\n",
      "[14:04:24] [OK] Tabela useall.staging_requisicoes carregada com sucesso\n",
      "[14:04:24] Preparando tabela useall.staging_estoque | Linhas: 134852\n",
      "[14:04:24] Iniciando COPY para useall.staging_estoque\n",
      "[14:04:33] [OK] Tabela useall.staging_estoque carregada com sucesso\n",
      "[14:04:33] Preparando tabela useall.staging_atendimentodereq | Linhas: 523187\n",
      "[14:04:33] Iniciando COPY para useall.staging_atendimentodereq\n",
      "[14:05:40] [OK] Tabela useall.staging_atendimentodereq carregada com sucesso\n",
      "[14:05:40] Preparando tabela useall.staging_solcompra | Linhas: 321\n",
      "[14:05:42] Iniciando COPY para useall.staging_solcompra\n",
      "[14:05:43] [OK] Tabela useall.staging_solcompra carregada com sucesso\n",
      "[14:05:43] Preparando tabela useall.staging_filiais | Linhas: 23\n",
      "[14:05:43] Iniciando COPY para useall.staging_filiais\n",
      "[14:05:43] [OK] Tabela useall.staging_filiais carregada com sucesso\n",
      "[14:05:43] Preparando tabela useall.staging_empresas | Linhas: 7\n",
      "[14:05:43] Iniciando COPY para useall.staging_empresas\n",
      "[14:05:43] [OK] Tabela useall.staging_empresas carregada com sucesso\n",
      "[14:05:43] Preparando tabela useall.staging_expedição | Linhas: 33946\n",
      "[14:05:43] Iniciando COPY para useall.staging_expedição\n",
      "[14:06:00] [OK] Tabela useall.staging_expedição carregada com sucesso\n",
      "[14:06:00] Preparando tabela useall.staging_clientesfornecedore | Linhas: 34156\n",
      "[14:06:01] Iniciando COPY para useall.staging_clientesfornecedore\n",
      "[14:06:20] [OK] Tabela useall.staging_clientesfornecedore carregada com sucesso\n",
      "[14:06:20] Preparando tabela useall.staging_almoxarifados | Linhas: 136\n",
      "[14:06:20] Iniciando COPY para useall.staging_almoxarifados\n",
      "[14:06:20] [OK] Tabela useall.staging_almoxarifados carregada com sucesso\n",
      "[14:06:20] --------------------------------------------------\n",
      "[14:06:20] 12 tabelas staging criadas com sucesso.\n",
      "[14:06:20] PROCESSO FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "def log(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "\n",
    "def copy_df_to_postgres(df, schema: str, table: str):\n",
    "    import psycopg2\n",
    "    import io\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(\n",
    "        buffer,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep=\"\\t\",\n",
    "        na_rep=\"\\\\N\"\n",
    "    )\n",
    "    buffer.seek(0)\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=os.getenv(\"PG_DBNAME\"),\n",
    "        user=os.getenv(\"PG_USER\"),\n",
    "        password=os.getenv(\"PG_PASSWORD\"),\n",
    "        host=os.getenv(\"PG_HOST\"),\n",
    "        port=os.getenv(\"PG_PORT\")\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        COPY {schema}.{table}\n",
    "        FROM STDIN\n",
    "        WITH (FORMAT CSV, DELIMITER E'\\t', NULL '\\\\N')\n",
    "    \"\"\"\n",
    "\n",
    "    cur.copy_expert(sql, buffer)\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "tabelas_criadas = 0\n",
    "dfs_nao_encontrados = []\n",
    "\n",
    "log(\"INICIANDO CARGA STAGING (COPY FROM)\")\n",
    "\n",
    "for df_nome in ordem_staging:\n",
    "    df = globals().get(df_nome)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        dfs_nao_encontrados.append(df_nome)\n",
    "        continue\n",
    "\n",
    "    tabela = \"staging_\" + df_nome.replace(\"dfuseall\", \"\")\n",
    "    tabela = tabela.lower()\n",
    "\n",
    "    log(f\"Preparando tabela {SCHEMA}.{tabela} | Linhas: {len(df)}\")\n",
    "\n",
    "    # 1️⃣ cria estrutura (DDL leve)\n",
    "    with engine.connect() as conn:\n",
    "        df.head(0).to_sql(\n",
    "            name=tabela,\n",
    "            con=conn,\n",
    "            schema=SCHEMA,\n",
    "            if_exists=\"replace\",\n",
    "            index=False\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "    log(f\"Iniciando COPY para {SCHEMA}.{tabela}\")\n",
    "\n",
    "    # 2️⃣ carga pesada via COPY\n",
    "    copy_df_to_postgres(df, SCHEMA, tabela)\n",
    "\n",
    "    log(f\"[OK] Tabela {SCHEMA}.{tabela} carregada com sucesso\")\n",
    "\n",
    "    tabelas_criadas += 1\n",
    "\n",
    "\n",
    "# ---------------- FINAL ----------------\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "\n",
    "if tabelas_criadas == 0:\n",
    "    log(\"Nenhuma tabela staging foi criada.\")\n",
    "    log(\"DataFrames não encontrados:\")\n",
    "    for nome in dfs_nao_encontrados:\n",
    "        log(f\" - {nome}\")\n",
    "else:\n",
    "    log(f\"{tabelas_criadas} tabelas staging criadas com sucesso.\")\n",
    "\n",
    "log(\"PROCESSO FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Silver definindo tipos automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:06:22] [SCHEMA] Usando schema_silver.json existente\n",
      "[14:06:22] Criando tabela silver useall.silver_custos\n",
      "[14:06:22] Criando tabela silver useall.silver_atendimentodereq\n",
      "[14:06:22] Criando tabela silver useall.silver_itens\n",
      "[14:06:22] Criando tabela silver useall.silver_unidades\n",
      "[14:06:22] Criando tabela silver useall.silver_cidades\n",
      "[14:06:22] Criando tabela silver useall.silver_requisicoes\n",
      "[14:06:22] Criando tabela silver useall.silver_estoque\n",
      "[14:06:22] Criando tabela silver useall.silver_solcompra\n",
      "[14:06:22] Criando tabela silver useall.silver_filiais\n",
      "[14:06:22] Criando tabela silver useall.silver_empresas\n",
      "[14:06:22] Criando tabela silver useall.silver_expedição\n",
      "[14:06:22] Criando tabela silver useall.silver_clientesfornecedore\n",
      "[14:06:23] Criando tabela silver useall.silver_almoxarifados\n",
      "[14:06:23] Carregando dados em useall.silver_custos\n",
      "[14:06:37] [OK] useall.silver_custos carregada\n",
      "[14:06:37] Carregando dados em useall.silver_atendimentodereq\n",
      "[14:07:06] [OK] useall.silver_atendimentodereq carregada\n",
      "[14:07:06] Carregando dados em useall.silver_itens\n",
      "[14:07:07] [OK] useall.silver_itens carregada\n",
      "[14:07:07] Carregando dados em useall.silver_unidades\n",
      "[14:07:07] [OK] useall.silver_unidades carregada\n",
      "[14:07:07] Carregando dados em useall.silver_cidades\n",
      "[14:07:07] [OK] useall.silver_cidades carregada\n",
      "[14:07:07] Carregando dados em useall.silver_requisicoes\n",
      "[14:08:11] [OK] useall.silver_requisicoes carregada\n",
      "[14:08:11] Carregando dados em useall.silver_estoque\n",
      "[14:08:18] [OK] useall.silver_estoque carregada\n",
      "[14:08:18] Carregando dados em useall.silver_solcompra\n",
      "[14:08:18] [OK] useall.silver_solcompra carregada\n",
      "[14:08:18] Carregando dados em useall.silver_filiais\n",
      "[14:08:18] [OK] useall.silver_filiais carregada\n",
      "[14:08:18] Carregando dados em useall.silver_empresas\n",
      "[14:08:18] [OK] useall.silver_empresas carregada\n",
      "[14:08:18] Carregando dados em useall.silver_expedição\n",
      "[14:08:21] [OK] useall.silver_expedição carregada\n",
      "[14:08:21] Carregando dados em useall.silver_clientesfornecedore\n",
      "[14:08:23] [OK] useall.silver_clientesfornecedore carregada\n",
      "[14:08:23] Carregando dados em useall.silver_almoxarifados\n",
      "[14:08:23] [OK] useall.silver_almoxarifados carregada\n",
      "[14:08:23] --------------------------------------------------\n",
      "[14:08:23] PROCESSO FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SCHEMA_FILE = Path(\"schema_silver.json\")\n",
    "\n",
    "SCHEMA = \"useall\"\n",
    "SAMPLE_LIMIT = 50000\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def load_or_create_schema(engine, schema, staging_tables):\n",
    "    # CASO 1 — schema já existe\n",
    "    if SCHEMA_FILE.exists():\n",
    "        log(\"[SCHEMA] Usando schema_silver.json existente\")\n",
    "        with open(SCHEMA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # CASO 2 — primeira execução → inferir\n",
    "    log(\"[SCHEMA] schema_silver.json não encontrado. Inferindo tipos...\")\n",
    "\n",
    "    schema_silver = {}\n",
    "\n",
    "    for staging_table in staging_tables:\n",
    "        silver_table = silver_table_name(staging_table)\n",
    "        log(f\"Profiling {schema}.{staging_table} -> {schema}.{silver_table}\")\n",
    "\n",
    "        df_sample = pd.read_sql(\n",
    "            f'SELECT * FROM {schema}.\"{staging_table}\" LIMIT {SAMPLE_LIMIT}',\n",
    "            engine\n",
    "        )\n",
    "\n",
    "        schema_silver[silver_table] = {\n",
    "            \"staging_table\": staging_table,\n",
    "            \"columns\": {\n",
    "                col.lower(): {\n",
    "                    **infer_column_type_final(df_sample[col]),\n",
    "                    \"source_col\": col\n",
    "                }\n",
    "                for col in df_sample.columns\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Salva schema congelado\n",
    "    with open(SCHEMA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(schema_silver, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    log(\"[SCHEMA] schema_silver.json criado e congelado\")\n",
    "    return schema_silver\n",
    "\n",
    "\n",
    "# Detecta formato de data\n",
    "def is_date_series(s: pd.Series):\n",
    "    sample = s.dropna().astype(str).head(50)\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d/%m/%Y %H:%M:%S\",\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            pd.to_datetime(sample, format=fmt)\n",
    "            return fmt\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# Inferência de tipo\n",
    "def infer_column_type_final(series: pd.Series) -> dict:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return {\"type\": \"text\"}\n",
    "\n",
    "    # BOOLEAN lógico\n",
    "    if s.astype(str).isin([\"0\",\"1\",\"true\",\"false\",\"True\",\"False\"]).all():\n",
    "        return {\"type\": \"boolean\"}\n",
    "\n",
    "    # DATE / TIMESTAMP\n",
    "    date_fmt = is_date_series(s)\n",
    "    if date_fmt:\n",
    "        return {\"type\": \"timestamp\", \"format\": date_fmt}\n",
    "\n",
    "    # INTEGER\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+\").all():\n",
    "        return {\"type\": \"bigint\"}\n",
    "\n",
    "    # DECIMAL\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+(\\.\\d+)?\").all():\n",
    "        return {\"type\": \"numeric(18,4)\"}\n",
    "\n",
    "    return {\"type\": \"text\"}\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = :schema\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE 'staging_%'\n",
    "    \"\"\"), {\"schema\": SCHEMA})\n",
    "\n",
    "    staging_tables = [row[0] for row in result.fetchall()]\n",
    "\n",
    "# Função nome silver\n",
    "def silver_table_name(staging_table: str) -> str:\n",
    "    return staging_table.replace(\"staging_\", \"silver_\")\n",
    "\n",
    "schema_silver = load_or_create_schema(\n",
    "    engine=engine,\n",
    "    schema=SCHEMA,\n",
    "    staging_tables=staging_tables\n",
    ")\n",
    "\n",
    "\n",
    "# Cria cast SQL\n",
    "def generate_cast_sql(col_dest, meta):\n",
    "    col_src = meta[\"source_col\"]\n",
    "\n",
    "    col_sql = f'\"{col_src}\"'\n",
    "    col_txt = f'{col_sql}::text'\n",
    "\n",
    "    if meta[\"type\"] == \"boolean\":\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN lower({col_txt}) IN ('1','true','sim','s','y','yes') THEN true\n",
    "            WHEN lower({col_txt}) IN ('0','false','nao','n','no') THEN false\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    if meta[\"type\"] == \"timestamp\":\n",
    "        fmt = meta.get(\"format\")\n",
    "\n",
    "        pg_fmt = {\n",
    "            \"%Y-%m-%d\": \"YYYY-MM-DD\",\n",
    "            \"%Y-%m-%d %H:%M:%S\": \"YYYY-MM-DD HH24:MI:SS\",\n",
    "            \"%Y-%m-%dT%H:%M:%S\": \"YYYY-MM-DD\\\"T\\\"HH24:MI:SS\",\n",
    "            \"%d/%m/%Y\": \"DD/MM/YYYY\",\n",
    "            \"%d/%m/%Y %H:%M:%S\": \"DD/MM/YYYY HH24:MI:SS\",\n",
    "        }.get(fmt)\n",
    "\n",
    "        if pg_fmt:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_txt} = '' THEN NULL\n",
    "                ELSE to_timestamp({col_txt}, '{pg_fmt}')\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_sql} IS NULL OR {col_txt} = '' THEN NULL\n",
    "                ELSE {col_sql}::timestamp\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "    if meta[\"type\"] in (\"bigint\",\"numeric(18,4)\"):\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN {col_txt} ~ '^-?\\\\d+(\\\\.\\\\d+)?$' THEN {col_txt}::{meta[\"type\"]}\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    return f'{col_sql}::text AS \"{col_dest}\"'\n",
    "\n",
    "# Gera CREATE TABLE\n",
    "def generate_create_table(schema, table, columns: dict):\n",
    "    cols = \",\\n  \".join(f'\"{col_dest}\" {meta[\"type\"]}' for col_dest, meta in columns.items())\n",
    "    return f\"\"\"\n",
    "    DROP TABLE IF EXISTS {schema}.\"{table}\";\n",
    "    CREATE TABLE {schema}.\"{table}\" (\n",
    "      {cols}\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "# Cria tabelas silver\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    log(f\"Criando tabela silver {SCHEMA}.{silver_table}\")\n",
    "    ddl = generate_create_table(SCHEMA, silver_table, meta[\"columns\"])\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl))\n",
    "\n",
    "# Gera INSERT\n",
    "def generate_insert_cast(staging_schema, SCHEMA, staging_table, silver_table, columns):\n",
    "    selects = \",\\n\".join(generate_cast_sql(col_dest, meta) for col_dest, meta in columns.items())\n",
    "    return f\"\"\"\n",
    "    INSERT INTO {SCHEMA}.\"{silver_table}\"\n",
    "    SELECT\n",
    "      {selects}\n",
    "    FROM {staging_schema}.\"{staging_table}\";\n",
    "    \"\"\"\n",
    "\n",
    "# Carrega dados\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    staging_table = meta[\"staging_table\"]\n",
    "    columns = meta[\"columns\"]\n",
    "    log(f\"Carregando dados em {SCHEMA}.{silver_table}\")\n",
    "    sql = generate_insert_cast(SCHEMA, SCHEMA, staging_table, silver_table, columns)\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        log(f\"[OK] {SCHEMA}.{silver_table} carregada\")\n",
    "    except Exception as e:\n",
    "        log(f\"[ERRO] {SCHEMA}.{silver_table} -> {e}\")\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "log(\"PROCESSO FINALIZADO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Gold - Adicionando novas colunas e agregando valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "DO $$\n",
    "DECLARE\n",
    "    r RECORD;\n",
    "    gold_table TEXT;\n",
    "BEGIN\n",
    "    FOR r IN\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'useall'\n",
    "          AND table_name LIKE 'silver_%'\n",
    "    LOOP\n",
    "\n",
    "        gold_table := replace(r.table_name, 'silver_', 'gold_');\n",
    "\n",
    "        -- CASO ESPECIAL: SILVER_REQUISICOES\n",
    "        IF r.table_name = 'silver_requisicoes' THEN\n",
    "\n",
    "            -- cria se não existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes\n",
    "                 WHERE false;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "        -- DEMAIS TABELAS\n",
    "        ELSE\n",
    "\n",
    "            -- cria se não existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT * FROM useall.%I WHERE false;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT * FROM useall.%I;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "        END IF;\n",
    "\n",
    "    END LOOP;\n",
    "END $$;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Dim_Calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ---------------- SQL ----------------\n",
    "sql_create_dim_calendario = text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS useall.dim_calendario (\n",
    "    data DATE PRIMARY KEY,\n",
    "\n",
    "    ano INT,\n",
    "    mes INT,\n",
    "    dia INT,\n",
    "\n",
    "    ano_mes TEXT,\n",
    "    ano_mes_atual TEXT,\n",
    "    ano_mes_ordem INT,\n",
    "\n",
    "    nome_mes TEXT,\n",
    "    nome_mes_abrev TEXT,\n",
    "    nome_dia TEXT,\n",
    "    nome_dia_abrev TEXT,\n",
    "\n",
    "    dia_semana INT,\n",
    "    semana_iso INT,\n",
    "    ano_iso INT,\n",
    "    trimestre INT,\n",
    "    \n",
    "    dia_mes_abr TEXT,\n",
    "\n",
    "    is_fim_de_semana BOOLEAN,\n",
    "    is_feriado BOOLEAN,\n",
    "    nome_feriado TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "sql_create_indices = text(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_data\n",
    "    ON useall.dim_calendario (data);\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_ano_mes_ordem\n",
    "    ON useall.dim_calendario (ano_mes_ordem);\n",
    "\"\"\")\n",
    "\n",
    "sql_atualiza_calendario = text(\"\"\"\n",
    "INSERT INTO useall.dim_calendario (\n",
    "    data,\n",
    "    ano,\n",
    "    mes,\n",
    "    dia,\n",
    "    ano_mes,\n",
    "    ano_mes_atual,\n",
    "    ano_mes_ordem,\n",
    "    nome_mes,\n",
    "    nome_mes_abrev,\n",
    "    nome_dia,\n",
    "    nome_dia_abrev,\n",
    "    dia_semana,\n",
    "    semana_iso,\n",
    "    ano_iso,\n",
    "    trimestre,\n",
    "    dia_mes_abr,\n",
    "    is_fim_de_semana,\n",
    "    is_feriado,\n",
    "    nome_feriado\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    d::date AS data,\n",
    "\n",
    "    EXTRACT(YEAR FROM d)::int AS ano,\n",
    "    EXTRACT(MONTH FROM d)::int AS mes,\n",
    "    EXTRACT(DAY FROM d)::int AS dia,\n",
    "\n",
    "    TO_CHAR(d, 'YYYY/MM') AS ano_mes,\n",
    "\n",
    "    CASE\n",
    "        WHEN EXTRACT(YEAR FROM d) = EXTRACT(YEAR FROM CURRENT_DATE)\n",
    "         AND EXTRACT(MONTH FROM d) = EXTRACT(MONTH FROM CURRENT_DATE)\n",
    "        THEN 'Mês Atual'\n",
    "        ELSE TO_CHAR(d, 'YYYY/MM')\n",
    "    END AS ano_mes_atual,\n",
    "\n",
    "    (EXTRACT(YEAR FROM d) * 100 + EXTRACT(MONTH FROM d))::int AS ano_mes_ordem,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Janeiro'\n",
    "        WHEN 2 THEN 'Fevereiro'\n",
    "        WHEN 3 THEN 'Março'\n",
    "        WHEN 4 THEN 'Abril'\n",
    "        WHEN 5 THEN 'Maio'\n",
    "        WHEN 6 THEN 'Junho'\n",
    "        WHEN 7 THEN 'Julho'\n",
    "        WHEN 8 THEN 'Agosto'\n",
    "        WHEN 9 THEN 'Setembro'\n",
    "        WHEN 10 THEN 'Outubro'\n",
    "        WHEN 11 THEN 'Novembro'\n",
    "        WHEN 12 THEN 'Dezembro'\n",
    "    END AS nome_mes,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Jan'\n",
    "        WHEN 2 THEN 'Fev'\n",
    "        WHEN 3 THEN 'Mar'\n",
    "        WHEN 4 THEN 'Abr'\n",
    "        WHEN 5 THEN 'Mai'\n",
    "        WHEN 6 THEN 'Jun'\n",
    "        WHEN 7 THEN 'Jul'\n",
    "        WHEN 8 THEN 'Ago'\n",
    "        WHEN 9 THEN 'Set'\n",
    "        WHEN 10 THEN 'Out'\n",
    "        WHEN 11 THEN 'Nov'\n",
    "        WHEN 12 THEN 'Dez'\n",
    "    END AS nome_mes_abrev,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Segunda-feira'\n",
    "        WHEN 2 THEN 'Terça-feira'\n",
    "        WHEN 3 THEN 'Quarta-feira'\n",
    "        WHEN 4 THEN 'Quinta-feira'\n",
    "        WHEN 5 THEN 'Sexta-feira'\n",
    "        WHEN 6 THEN 'Sábado'\n",
    "        WHEN 7 THEN 'Domingo'\n",
    "    END AS nome_dia,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Seg'\n",
    "        WHEN 2 THEN 'Ter'\n",
    "        WHEN 3 THEN 'Qua'\n",
    "        WHEN 4 THEN 'Qui'\n",
    "        WHEN 5 THEN 'Sex'\n",
    "        WHEN 6 THEN 'Sáb'\n",
    "        WHEN 7 THEN 'Dom'\n",
    "    END AS nome_dia_abrev,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d)::int AS dia_semana,\n",
    "    EXTRACT(WEEK FROM d)::int AS semana_iso,\n",
    "    EXTRACT(ISOYEAR FROM d)::int AS ano_iso,\n",
    "    EXTRACT(QUARTER FROM d)::int AS trimestre,\n",
    "\n",
    "    TO_CHAR(d, 'DD/MM') AS dia_mes_abr,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d) IN (6,7) AS is_fim_de_semana,\n",
    "    FALSE AS is_feriado,\n",
    "    NULL AS nome_feriado\n",
    "FROM (\n",
    "    SELECT DISTINCT data::date AS d\n",
    "    FROM useall.gold_requisicoes\n",
    "    WHERE data IS NOT NULL\n",
    ") x\n",
    "ON CONFLICT (data) DO NOTHING;\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- EXECUÇÃO ----------------\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql_create_dim_calendario)\n",
    "    conn.execute(sql_create_indices)\n",
    "    conn.execute(sql_atualiza_calendario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Definir a raiz do projeto relativa a este arquivo script\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Path(__file__).parent pega a pasta onde o seu script .py está\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Montar o caminho relativo para o SQL\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Isso vai funcionar em Windows (\\\\) e Linux (/) automaticamente\u001b[39;00m\n\u001b[0;32m     11\u001b[0m sql_file_path \u001b[38;5;241m=\u001b[39m BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvw_gold_filiais_uf.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sqlalchemy import text\n",
    "\n",
    "# 1. Definir a raiz do projeto relativa a este arquivo script\n",
    "# Path(__file__).parent pega a pasta onde o seu script .py está\n",
    "BASE_DIR = Path(__file__).resolve().parent\n",
    "\n",
    "# 2. Montar o caminho relativo para o SQL\n",
    "# Isso vai funcionar em Windows (\\\\) e Linux (/) automaticamente\n",
    "sql_file_path = BASE_DIR / \"sql\" / \"view\" / \"vw_gold_filiais_uf.sql\"\n",
    "\n",
    "def rodar_script_sql(engine, caminho):\n",
    "    # Verificar se o arquivo realmente existe no caminho relativo\n",
    "    if not caminho.exists():\n",
    "        print(f\"❌ Erro: Arquivo não encontrado!\")\n",
    "        print(f\"   Buscado em: {caminho}\")\n",
    "        print(f\"   Diretório atual de execução: {os.getcwd()}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Lendo o conteúdo do SQL\n",
    "        query = caminho.read_text(encoding='utf-8')\n",
    "\n",
    "        # Executando no banco\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(query))\n",
    "            print(f\"✅ View processada com sucesso via caminho relativo!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro na execução: {e}\")\n",
    "\n",
    "# Chamar a função\n",
    "rodar_script_sql(engine, sql_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
