{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de API Useall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline iniciada em 28/01/2026 10:34:42 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "BASE_URL = os.getenv(\"USEALL_BASE_URL\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"use-relatorio-token\": os.getenv(\"USEALL_TOKEN\"),\n",
    "}\n",
    "\n",
    "\n",
    "def buscar_dados_api(\n",
    "    identificacao, nome_arquivo, backend_filters=None, extra_params=None\n",
    "):\n",
    "    \"\"\"Busca dados na API UseAll e retorna um DataFrame (ou None em caso de erro/vazio)\"\"\"\n",
    "\n",
    "    query_params = {\"Identificacao\": identificacao}\n",
    "\n",
    "    if backend_filters:\n",
    "        query_params[\"FiltrosSqlQuery\"] = json.dumps(\n",
    "            backend_filters, ensure_ascii=False\n",
    "        )\n",
    "\n",
    "    if extra_params:\n",
    "        query_params.update(extra_params)\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Iniciando extração: {nome_arquivo}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                BASE_URL, headers=HEADERS, params=query_params, timeout=500\n",
    "            )\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                print(\n",
    "                    f\"[{time.strftime('%H:%M:%S')}] Erro 429 (Too Many Requests) em {nome_arquivo}. Aguardando 185 segundos...\"\n",
    "                )\n",
    "                time.sleep(185)\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            return df\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Timeout atingido para {nome_arquivo}. Aguardando 185 segundos...\"\n",
    "            )\n",
    "            time.sleep(185)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Erro irrecuperável em {nome_arquivo}: {str(e)}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "def buscar_dados_api_post(nome_arquivo, payload):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Iniciando extração: {nome_arquivo}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                BASE_URL,\n",
    "                headers=HEADERS,\n",
    "                json=payload,\n",
    "                timeout=500,\n",
    "            )\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                print(\n",
    "                    f\"[{time.strftime('%H:%M:%S')}] Erro 429 em {nome_arquivo}. Aguardando 185 segundos...\"\n",
    "                )\n",
    "                time.sleep(185)\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            return pd.DataFrame(data)\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Timeout em {nome_arquivo}. Aguardando 185 segundos...\"\n",
    "            )\n",
    "            time.sleep(185)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Erro irrecuperável em {nome_arquivo}: {str(e)}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "def salvar_parquet(df, nome_arquivo):\n",
    "    \"\"\"Salva o DataFrame em arquivo parquet\"\"\"\n",
    "    if df is not None and not df.empty:\n",
    "        # Garante extensão .parquet\n",
    "        if not nome_arquivo.endswith(\".parquet\"):\n",
    "            nome_arquivo += \".parquet\"\n",
    "\n",
    "        try:\n",
    "            df.to_parquet(nome_arquivo, index=False)\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Sucesso ao salvar: {nome_arquivo} (Linhas: {len(df)})\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"[{time.strftime('%H:%M:%S')}] Erro ao salvar {nome_arquivo}: {str(e)}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            f\"[{time.strftime('%H:%M:%S')}] Nada a salvar para {nome_arquivo} (DataFrame vazio ou None)\"\n",
    "        )\n",
    "\n",
    "\n",
    "def verificar_tipos_dados():\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] VERIFICAÇÃO DE TIPOS DE DADOS\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    encontrou = False\n",
    "\n",
    "    for nome, obj in globals().items():\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            encontrou = True\n",
    "            print(f\"\\nDataFrame: {nome}\")\n",
    "            if not obj.empty:\n",
    "                print(\"-\" * 30)\n",
    "                print(obj.dtypes)\n",
    "                print(\"-\" * 30)\n",
    "            else:\n",
    "                print(\"  (DataFrame vazio)\")\n",
    "\n",
    "    if not encontrou:\n",
    "        print(\"Nenhum DataFrame encontrado em memória.\")\n",
    "\n",
    "\n",
    "# --- Defines Auxiliares de Filtro ---\n",
    "def filtro_simples(nome, valor):\n",
    "    return {\n",
    "        \"Nome\": nome,\n",
    "        \"Valor\": valor,\n",
    "        \"Operador\": None,\n",
    "        \"Descricao\": None,\n",
    "        \"ValorFormatado\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "def carregar_dfs_globais(tarefas):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO CARGA EM MEMÓRIA ---\")\n",
    "\n",
    "    for t in tarefas:\n",
    "        nome = t[\"nome\"]\n",
    "        df = buscar_dados_api(t[\"id\"], nome, t.get(\"filtros\"), t.get(\"extra_params\"))\n",
    "\n",
    "        if df is not None:\n",
    "            globals()[nome] = df\n",
    "        else:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "\n",
    "def carregar_tarefa_complexa(tarefa):\n",
    "    nome = tarefa[\"nome\"]\n",
    "\n",
    "    df = buscar_dados_api(\n",
    "        tarefa[\"id\"], nome, tarefa.get(\"filtros\"), tarefa.get(\"extra_params\")\n",
    "    )\n",
    "\n",
    "    if df is not None:\n",
    "        globals()[nome] = df\n",
    "    else:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "\n",
    "pipeline_start = time.time()\n",
    "print(f\"--- Pipeline iniciada em {time.strftime('%d/%m/%Y %H:%M:%S')} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis de filtros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fixos = {\"pagina\": 1, \"qtderegistros\": 1}\n",
    "\n",
    "tarefas_simples = [\n",
    "    {\n",
    "        \"nome\": \"dfuseallitens\",\n",
    "        \"id\": \"m2_estoque_item\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallunidades\",\n",
    "        \"id\": \"m2_estoque_unidade\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsegmentos\",\n",
    "        \"id\": \"m2_vendas_segmento\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallcidades\",\n",
    "        \"id\": \"m2_geral_cidades\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsolcompra\",\n",
    "        \"id\": \"m2_compras_m2_compras_solicitacao_de_compras__extra\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DataFim\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallfiliais\",\n",
    "        \"id\": \"m2_geral_filiais\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/1900, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2027, 14:00:00\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallempresas\",\n",
    "        \"id\": \"m2_geral_empresas\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/2022, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2027, 14:00:00\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallexpedição\",\n",
    "        \"id\": \"m2_vendas_extracao_de_dados__saida_expedicao\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"data1\", \"01/01/1900\"),\n",
    "            filtro_simples(\"data2\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallclientesfornecedore\",\n",
    "        \"id\": \"m2_geral_clientes__fornecedores\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallalmoxarifados\",\n",
    "        \"id\": \"m2_estoque_almoxarifados\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2027\"),\n",
    "        ],\n",
    "        \"extra_params\": params_fixos,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPLEXAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtros_req = [\n",
    "    {\n",
    "        \"Nome\": \"IDFILIAL\",\n",
    "        \"Valor\": [\n",
    "            333,\n",
    "            339,\n",
    "            340,\n",
    "            381,\n",
    "            389,\n",
    "            336,\n",
    "            387,\n",
    "            520,\n",
    "            404,\n",
    "            558,\n",
    "            578,\n",
    "            341,\n",
    "            390,\n",
    "            345,\n",
    "            344,\n",
    "            346,\n",
    "            335,\n",
    "            334,\n",
    "            342,\n",
    "            343,\n",
    "        ],\n",
    "        \"Operador\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"DATA\",\n",
    "        \"Valor\": \"01/01/2010,01/01/2027\",\n",
    "        \"Operador\": 8,\n",
    "        \"TipoPeriodoData\": 5,\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"DATAPREVATEND\",\n",
    "        \"Valor\": \"01/01/2010,01/01/2027\",\n",
    "        \"Operador\": 8,\n",
    "        \"TipoPeriodoData\": 8,\n",
    "    },\n",
    "    {\"Nome\": \"CLASSGRUPOITEM\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"CLASSCONTACDC\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"quebra\", \"Valor\": 1},\n",
    "    {\"Nome\": \"FILTROSWHERE\", \"Valor\": \" AND IDEMPRESA = 211\"},\n",
    "]\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "filtros_estoque = [\n",
    "    {\"Nome\": \"ADDATA\", \"Valor\": datetime.now().strftime(\"%d/%m/%Y\")},\n",
    "    {\"Nome\": \"ANQUEBRA\", \"Valor\": 0},\n",
    "    {\"Nome\": \"FILTROSWHEREFORNEC\", \"Valor\": \"\"},\n",
    "    {\n",
    "        \"Nome\": \"FILTROSREGISTROSATIVO\",\n",
    "        \"Valor\": \" AND ITEM.ATIVO = 1 AND ALMOX.ATIVO = 1 AND ITEMALMOX.ATIVO = 1\",\n",
    "    },\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": \" AND EXISTS(SELECT 1 FROM USE_USUARIOS_FILIAIS UFILIAIS \"\n",
    "        \"WHERE UFILIAIS.IDEMPRESA = T.IDEMPRESA \"\n",
    "        \"AND UFILIAIS.IDFILIAL = T.IDFILIAL \"\n",
    "        \"AND UFILIAIS.IDUSUARIO = 7332) \"\n",
    "        \"AND T.IDFILIAL in (333,339,340,381,389,336,387,520,404,558,578,341,390,345,344,346,335,334,342,343)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "filtros_atend = [\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": (\n",
    "            \"WHERE IDEMPRESA = 211 \"\n",
    "            \"AND IDFILIAL IN (333,339, 340, 381, 389, 336, 387, 520, 404, 558, 578, 341, 390, 345, 344, 346, 335, 334, 342, 343) \"\n",
    "            \"AND DATA_REQ BETWEEN '01/01/1900' AND '01/01/2900' \"\n",
    "            \"AND DATA_ATEND BETWEEN '01/01/1900' AND '01/01/2900'\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "params_atend = {\n",
    "    \"NomeOrganizacao\": \"SETUP SERVICOS ESPECIALIZADOS LTDA\",\n",
    "    \"Parametros\": json.dumps(\n",
    "        [{\"Nome\": \"usecellmerging\", \"Valor\": True}, {\"Nome\": \"quebra\", \"Valor\": 0}]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 1 — REQUISIÇÕES\n",
    "# ===============================\n",
    "\n",
    "tarefa_requisicoes = {\n",
    "    \"nome\": \"dfuseallrequisicoes\",\n",
    "    \"id\": \"m2_estoque_requisicao_de_materiais\",\n",
    "    \"filtros\": filtros_req,\n",
    "    \"extra_params\": None,\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 2 — ESTOQUE\n",
    "# ===============================\n",
    "\n",
    "tarefa_estoque = {\n",
    "    \"nome\": \"dfuseallestoque\",\n",
    "    \"id\": \"m2_estoque_saldo_de_estoque\",\n",
    "    \"filtros\": filtros_estoque,\n",
    "    \"extra_params\": None,\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 3 — ATENDIMENTO DE REQUISIÇÕES\n",
    "# ===============================\n",
    "\n",
    "tarefa_atendimento = {\n",
    "    \"nome\": \"dfuseallatendimentodereq\",\n",
    "    \"id\": \"m2_estoque_atendimentos_de_requisicao\",\n",
    "    \"filtros\": filtros_atend,\n",
    "    \"extra_params\": params_atend,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando funções\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:59:20] --- INICIANDO CARGA EM MEMÓRIA ---\n",
      "[07:59:20] Iniciando extração: dfuseallitens...\n",
      "[07:59:22] Erro 429 (Too Many Requests) em dfuseallitens. Aguardando 185 segundos...\n",
      "[08:02:29] Iniciando extração: dfuseallunidades...\n",
      "[08:02:30] Iniciando extração: dfuseallsegmentos...\n",
      "[08:02:31] Iniciando extração: dfuseallcidades...\n",
      "[08:02:32] Iniciando extração: dfuseallsolcompra...\n",
      "[08:02:33] Iniciando extração: dfuseallfiliais...\n",
      "[08:02:35] Iniciando extração: dfuseallempresas...\n",
      "[08:02:36] Iniciando extração: dfuseallexpedição...\n",
      "[08:02:52] Iniciando extração: dfuseallclientesfornecedore...\n",
      "[08:03:01] Iniciando extração: dfuseallalmoxarifados...\n"
     ]
    }
   ],
   "source": [
    "carregar_dfs_globais(tarefas_simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:03:02] Iniciando extração: dfuseallestoque...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_estoque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:03:17] Iniciando extração: dfuseallatendimentodereq...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_atendimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:08:53] Iniciando extração: dfuseallrequisicoes...\n"
     ]
    }
   ],
   "source": [
    "carregar_tarefa_complexa(tarefa_requisicoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api Custos - Particularidade de loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 08:11:45 | INFO | INÍCIO DO PIPELINE\n",
      "2026-01-28 08:11:45 | INFO | Raw do dia não encontrado. Iniciando download por grupos.\n",
      "2026-01-28 08:11:45 | INFO | INÍCIO GRUPO 1/6: ctfm\n",
      "2026-01-28 08:11:45 | INFO | Requisição API | Grupo ctfm\n",
      "2026-01-28 08:12:20 | INFO | Resposta API OK | Grupo ctfm | duração: 35.38s\n",
      "2026-01-28 08:12:23 | INFO | Arquivo salvo | data/raw/custos/ctfm.parquet | registros: 21723 | duração: 2.01s\n",
      "2026-01-28 08:12:23 | INFO | FIM GRUPO: ctfm | duração: 38.22s\n",
      "2026-01-28 08:12:23 | INFO | Cooldown 185s antes do próximo grupo\n",
      "2026-01-28 08:15:28 | INFO | INÍCIO GRUPO 2/6: lojas\n",
      "2026-01-28 08:15:29 | INFO | Requisição API | Grupo lojas\n",
      "2026-01-28 08:16:06 | INFO | Resposta API OK | Grupo lojas | duração: 37.47s\n",
      "2026-01-28 08:16:10 | INFO | Arquivo salvo | data/raw/custos/lojas.parquet | registros: 26646 | duração: 0.59s\n",
      "2026-01-28 08:16:10 | INFO | FIM GRUPO: lojas | duração: 41.84s\n",
      "2026-01-28 08:16:10 | INFO | Cooldown 185s antes do próximo grupo\n",
      "2026-01-28 08:19:15 | INFO | INÍCIO GRUPO 3/6: vm\n",
      "2026-01-28 08:19:15 | INFO | Requisição API | Grupo vm\n",
      "2026-01-28 08:19:52 | INFO | Resposta API OK | Grupo vm | duração: 36.35s\n",
      "2026-01-28 08:19:52 | INFO | Arquivo salvo | data/raw/custos/vm.parquet | registros: 22769 | duração: 0.15s\n",
      "2026-01-28 08:19:52 | INFO | FIM GRUPO: vm | duração: 37.08s\n",
      "2026-01-28 08:19:52 | INFO | Cooldown 185s antes do próximo grupo\n",
      "2026-01-28 08:22:57 | INFO | INÍCIO GRUPO 4/6: setup_automacao\n",
      "2026-01-28 08:22:57 | INFO | Requisição API | Grupo setup_automacao\n",
      "2026-01-28 08:23:37 | INFO | Resposta API OK | Grupo setup_automacao | duração: 39.34s\n",
      "2026-01-28 08:23:39 | INFO | Arquivo salvo | data/raw/custos/setup_automacao.parquet | registros: 97952 | duração: 0.52s\n",
      "2026-01-28 08:23:39 | INFO | FIM GRUPO: setup_automacao | duração: 41.56s\n",
      "2026-01-28 08:23:39 | INFO | Cooldown 185s antes do próximo grupo\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# IMPORTS E ENV\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "BASE_URL = os.getenv(\"USEALL_BASE_URL\")\n",
    "TOKEN = os.getenv(\"USEALL_TOKEN\")\n",
    "\n",
    "IDENTIFICACAO = \"m2_estoque_custos\"\n",
    "DATA_REF = datetime.now(ZoneInfo(\"America/Sao_Paulo\")).strftime(\"%d/%m/%Y\")\n",
    "ESPERA = 185\n",
    "\n",
    "HEADERS = {\"accept\": \"application/json\", \"use-relatorio-token\": TOKEN}\n",
    "\n",
    "# %%\n",
    "# LOGGING (Jupyter-safe)\n",
    "logger = logging.getLogger(\"useall_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s | %(levelname)s | %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# %%\n",
    "# PATHS\n",
    "RAW_DIR = \"data/raw/custos\"\n",
    "RAW_FINAL = \"data/staging_custos_raw.parquet\"\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "# FUNÇÕES AUXILIARES\n",
    "def ja_baixado_hoje(path: str) -> bool:\n",
    "    if not os.path.exists(path):\n",
    "        return False\n",
    "    mod_time = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "    return mod_time.date() == datetime.today().date()\n",
    "\n",
    "\n",
    "def log_etapa(msg: str, inicio: float | None = None) -> float:\n",
    "    agora = time.time()\n",
    "    if inicio:\n",
    "        logger.info(f\"{msg} | duração: {agora - inicio:.2f}s\")\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    return agora\n",
    "\n",
    "\n",
    "# %%\n",
    "# GRUPOS\n",
    "grupos = {\n",
    "    \"ctfm\": [342, 343],\n",
    "    \"lojas\": [335, 334],\n",
    "    \"vm\": [345, 344, 346],\n",
    "    \"setup_automacao\": [333],\n",
    "    \"servicos\": [339, 340, 381, 389],\n",
    "    \"setup\": [336, 387, 520, 404, 558, 578, 341, 390],\n",
    "}\n",
    "\n",
    "\n",
    "# %%\n",
    "# PIPELINE\n",
    "pipeline_start = log_etapa(\"INÍCIO DO PIPELINE\")\n",
    "\n",
    "if ja_baixado_hoje(RAW_FINAL):\n",
    "    logger.info(\"Raw consolidado já existe e é de hoje. Pipeline encerrado.\")\n",
    "else:\n",
    "    logger.info(\"Raw do dia não encontrado. Iniciando download por grupos.\")\n",
    "\n",
    "    nomes_grupos = list(grupos.items())\n",
    "    total_grupos = len(nomes_grupos)\n",
    "\n",
    "    for idx, (nome, ids) in enumerate(nomes_grupos, start=1):\n",
    "        ultimo_grupo = idx == total_grupos\n",
    "        grupo_start = log_etapa(f\"INÍCIO GRUPO {idx}/{total_grupos}: {nome}\")\n",
    "\n",
    "        raw_path = f\"{RAW_DIR}/{nome}.parquet\"\n",
    "\n",
    "        if ja_baixado_hoje(raw_path):\n",
    "            logger.info(f\"{nome} já existe e é de hoje. Pulando grupo.\")\n",
    "            continue\n",
    "\n",
    "        filtros = [\n",
    "            {\"Nome\": \"idfilial\", \"Valor\": ids, \"Operador\": 1},\n",
    "            {\"Nome\": \"FILTROSREGISTROSATIVO\", \"Valor\": \"\"},\n",
    "            {\n",
    "                \"Nome\": \"filtroswhere\",\n",
    "                \"Valor\": f\" AND IDFILIAL IN ({','.join(map(str, ids))})\",\n",
    "            },\n",
    "            {\"Nome\": \"data\", \"Valor\": DATA_REF},\n",
    "        ]\n",
    "\n",
    "        params = {\n",
    "            \"Identificacao\": IDENTIFICACAO,\n",
    "            \"FiltrosSqlQuery\": json.dumps(filtros, ensure_ascii=False),\n",
    "        }\n",
    "\n",
    "        request_start = log_etapa(f\"Requisição API | Grupo {nome}\")\n",
    "\n",
    "        while True:\n",
    "            r = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=180)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                log_etapa(f\"Resposta API OK | Grupo {nome}\", request_start)\n",
    "\n",
    "                payload = r.json()\n",
    "                registros = (\n",
    "                    payload.get(\"data\") if isinstance(payload, dict) else payload\n",
    "                )\n",
    "\n",
    "                if registros:\n",
    "                    for row in registros:\n",
    "                        row[\"_grupo_origem\"] = nome\n",
    "\n",
    "                    df = pd.DataFrame(registros)\n",
    "\n",
    "                    save_start = time.time()\n",
    "                    df.to_parquet(raw_path, engine=\"pyarrow\", index=False)\n",
    "                    log_etapa(\n",
    "                        f\"Arquivo salvo | {raw_path} | registros: {len(df)}\", save_start\n",
    "                    )\n",
    "                else:\n",
    "                    logger.warning(f\"Grupo {nome} retornou 0 registros\")\n",
    "\n",
    "                break\n",
    "\n",
    "            if r.status_code == 429:\n",
    "                logger.warning(f\"429 Rate limit | {nome} | aguardando {ESPERA}s\")\n",
    "                time.sleep(ESPERA)\n",
    "                continue\n",
    "\n",
    "            if r.status_code == 400:\n",
    "                logger.error(f\"400 Payload pesado | Grupo {nome}\")\n",
    "                break\n",
    "\n",
    "            r.raise_for_status()\n",
    "\n",
    "        log_etapa(f\"FIM GRUPO: {nome}\", grupo_start)\n",
    "\n",
    "        # espera somente se NÃO for o último grupo\n",
    "        if not ultimo_grupo:\n",
    "            logger.info(f\"Cooldown {ESPERA}s antes do próximo grupo\")\n",
    "            time.sleep(ESPERA)\n",
    "\n",
    "log_etapa(\"FIM DO PIPELINE\", pipeline_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_registros = []\n",
    "\n",
    "for arquivo in os.listdir(RAW_DIR):\n",
    "    if arquivo.endswith(\".parquet\"):\n",
    "        path = os.path.join(RAW_DIR, arquivo)\n",
    "\n",
    "        df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "        # origem = nome do arquivo sem extensão\n",
    "        grupo_origem = os.path.splitext(arquivo)[0]\n",
    "        df[\"_grupo_origem\"] = grupo_origem\n",
    "\n",
    "        todos_registros.append(df)\n",
    "        logging.info(f\"Lido {arquivo} ({len(df)} registros)\")\n",
    "\n",
    "if todos_registros:\n",
    "    df_final = pd.concat(todos_registros, ignore_index=True)\n",
    "    df_final.to_parquet(RAW_FINAL, engine=\"pyarrow\", index=False)\n",
    "    logging.info(f\"Arquivo consolidado criado: {RAW_FINAL} ({len(df_final)} registros)\")\n",
    "else:\n",
    "    logging.info(\"Nenhum arquivo Parquet encontrado em RAW_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api Estoques - Consolida diariamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDSECAO</th>\n",
       "      <th>IDFILIAL</th>\n",
       "      <th>IDEMPRESA</th>\n",
       "      <th>IDALMOX</th>\n",
       "      <th>TIPOALMOX</th>\n",
       "      <th>DESC_ALMOX</th>\n",
       "      <th>IDUN</th>\n",
       "      <th>IDITEM</th>\n",
       "      <th>IDENTIFICACAO</th>\n",
       "      <th>DESC_ITEM</th>\n",
       "      <th>...</th>\n",
       "      <th>ESTOQUEMAX</th>\n",
       "      <th>DESC_TIPOITEM</th>\n",
       "      <th>SIGLA</th>\n",
       "      <th>RAZAOSOCIAL_FORNECEDOR</th>\n",
       "      <th>DESATIVACAO</th>\n",
       "      <th>COMPRASOBENCOMENDA</th>\n",
       "      <th>PONTOPEDIDO</th>\n",
       "      <th>POSICAO</th>\n",
       "      <th>DESC_SECAO</th>\n",
       "      <th>data_referencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>211</td>\n",
       "      <td>2515</td>\n",
       "      <td>0</td>\n",
       "      <td>MERC. MATRIZ</td>\n",
       "      <td>1818</td>\n",
       "      <td>401967</td>\n",
       "      <td>10009</td>\n",
       "      <td>CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mercadoria para revenda</td>\n",
       "      <td>UN</td>\n",
       "      <td>OTAVIO ROCKENBACH NEUTZLING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>211</td>\n",
       "      <td>2535</td>\n",
       "      <td>0</td>\n",
       "      <td>MERC. EM GARANTIA MATRIZ</td>\n",
       "      <td>1818</td>\n",
       "      <td>401967</td>\n",
       "      <td>10009</td>\n",
       "      <td>CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mercadoria para revenda</td>\n",
       "      <td>UN</td>\n",
       "      <td>OTAVIO ROCKENBACH NEUTZLING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>211</td>\n",
       "      <td>2537</td>\n",
       "      <td>0</td>\n",
       "      <td>MERC. COMODATO MATRIZ</td>\n",
       "      <td>1818</td>\n",
       "      <td>401967</td>\n",
       "      <td>10009</td>\n",
       "      <td>CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mercadoria para revenda</td>\n",
       "      <td>UN</td>\n",
       "      <td>OTAVIO ROCKENBACH NEUTZLING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>211</td>\n",
       "      <td>2544</td>\n",
       "      <td>0</td>\n",
       "      <td>CAIXA PLANTÃO</td>\n",
       "      <td>1818</td>\n",
       "      <td>401967</td>\n",
       "      <td>10009</td>\n",
       "      <td>CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mercadoria para revenda</td>\n",
       "      <td>UN</td>\n",
       "      <td>OTAVIO ROCKENBACH NEUTZLING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>211</td>\n",
       "      <td>4526</td>\n",
       "      <td>0</td>\n",
       "      <td>MOBILIZAÇÃO CD MATRIZ</td>\n",
       "      <td>1818</td>\n",
       "      <td>401967</td>\n",
       "      <td>10009</td>\n",
       "      <td>CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mercadoria para revenda</td>\n",
       "      <td>UN</td>\n",
       "      <td>OTAVIO ROCKENBACH NEUTZLING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDSECAO  IDFILIAL  IDEMPRESA  IDALMOX  TIPOALMOX                DESC_ALMOX  \\\n",
       "0      NaN       333        211     2515          0              MERC. MATRIZ   \n",
       "1      NaN       333        211     2535          0  MERC. EM GARANTIA MATRIZ   \n",
       "2      NaN       333        211     2537          0     MERC. COMODATO MATRIZ   \n",
       "3      NaN       333        211     2544          0             CAIXA PLANTÃO   \n",
       "4      NaN       333        211     4526          0     MOBILIZAÇÃO CD MATRIZ   \n",
       "\n",
       "   IDUN  IDITEM IDENTIFICACAO  \\\n",
       "0  1818  401967         10009   \n",
       "1  1818  401967         10009   \n",
       "2  1818  401967         10009   \n",
       "3  1818  401967         10009   \n",
       "4  1818  401967         10009   \n",
       "\n",
       "                                           DESC_ITEM  ...  ESTOQUEMAX  \\\n",
       "0  CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF   ...         0.0   \n",
       "1  CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF   ...         0.0   \n",
       "2  CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF   ...         0.0   \n",
       "3  CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF   ...         0.0   \n",
       "4  CONECTOR CAMPO CLICK SC/A P/FIBRA OPTICA APX-XFF   ...         0.0   \n",
       "\n",
       "             DESC_TIPOITEM  SIGLA       RAZAOSOCIAL_FORNECEDOR DESATIVACAO  \\\n",
       "0  mercadoria para revenda     UN  OTAVIO ROCKENBACH NEUTZLING           0   \n",
       "1  mercadoria para revenda     UN  OTAVIO ROCKENBACH NEUTZLING           0   \n",
       "2  mercadoria para revenda     UN  OTAVIO ROCKENBACH NEUTZLING           0   \n",
       "3  mercadoria para revenda     UN  OTAVIO ROCKENBACH NEUTZLING           0   \n",
       "4  mercadoria para revenda     UN  OTAVIO ROCKENBACH NEUTZLING           0   \n",
       "\n",
       "  COMPRASOBENCOMENDA  PONTOPEDIDO  POSICAO  DESC_SECAO  data_referencia  \n",
       "0                  0          NaN      1.0        None       2026-01-01  \n",
       "1                  0          NaN      2.0        None       2026-01-01  \n",
       "2                  0          NaN      3.0        None       2026-01-01  \n",
       "3                  0          NaN      4.0        None       2026-01-01  \n",
       "4                  0          0.0      5.0        None       2026-01-01  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def coletar_e_consolidar_estoque_matriz():\n",
    "    data_inicio = datetime.strptime(\"01/01/2026\", \"%d/%m/%Y\")\n",
    "    data_fim = datetime.now()\n",
    "\n",
    "    pasta_raw = Path(\"data/raw/estoque\")\n",
    "    pasta_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data_atual = data_inicio\n",
    "\n",
    "    while data_atual <= data_fim:\n",
    "        data_br = data_atual.strftime(\"%d/%m/%Y\")\n",
    "        data_iso = data_atual.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        arquivo = pasta_raw / f\"estoque_{data_iso}.parquet\"\n",
    "        if arquivo.exists():\n",
    "            data_atual += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        filtros_estoque = [\n",
    "            {\"Nome\": \"ADDATA\", \"Valor\": data_br},\n",
    "            {\"Nome\": \"ANQUEBRA\", \"Valor\": 0},\n",
    "            {\"Nome\": \"FILTROSWHEREFORNEC\", \"Valor\": \"\"},\n",
    "            {\n",
    "                \"Nome\": \"FILTROSREGISTROSATIVO\",\n",
    "                \"Valor\": \" AND ITEM.ATIVO = 1 AND ALMOX.ATIVO = 1 AND ITEMALMOX.ATIVO = 1\",\n",
    "            },\n",
    "            {\n",
    "                \"Nome\": \"FILTROSWHERE\",\n",
    "                \"Valor\": \" AND EXISTS(SELECT 1 FROM USE_USUARIOS_FILIAIS UFILIAIS \"\n",
    "                \"WHERE UFILIAIS.IDEMPRESA = T.IDEMPRESA \"\n",
    "                \"AND UFILIAIS.IDFILIAL = T.IDFILIAL \"\n",
    "                \"AND UFILIAIS.IDUSUARIO = 7332) \"\n",
    "                \"AND T.IDFILIAL in (333)\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        df = buscar_dados_api(\n",
    "            identificacao=\"m2_estoque_saldo_de_estoque\",\n",
    "            nome_arquivo=f\"estoque_{data_iso}\",\n",
    "            backend_filters=filtros_estoque,\n",
    "        )\n",
    "\n",
    "        if df is not None and not df.empty:\n",
    "            df[\"data_referencia\"] = data_iso\n",
    "            df.to_parquet(arquivo, index=False)\n",
    "\n",
    "        data_atual += timedelta(days=1)\n",
    "\n",
    "    arquivos = sorted(pasta_raw.glob(\"estoque_*.parquet\"))\n",
    "    if not arquivos:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_final = pd.concat(\n",
    "        [pd.read_parquet(a) for a in arquivos],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    df_final.to_parquet(\n",
    "        \"data/staging_estoque_day_raw.parquet\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# ===============================\n",
    "# EXECUÇÃO\n",
    "# ===============================\n",
    "\n",
    "dfuseallestoque_diario_matriz = coletar_e_consolidar_estoque_matriz()\n",
    "\n",
    "dfuseallestoque_diario_matriz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando Tipos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Verificação de Tipos ---\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO VERIFICAÇÃO DE TIPOS ---\")\n",
    "verificar_tipos_dados()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações Banco de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando .env\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "user = quote(os.getenv(\"PG_USER\"))\n",
    "password = quote(os.getenv(\"PG_PASSWORD\"))\n",
    "host = os.getenv(\"PG_HOST\")\n",
    "port = os.getenv(\"PG_PORT\")\n",
    "dbname = os.getenv(\"PG_DBNAME\")\n",
    "\n",
    "SCHEMA = os.getenv(\"DB_SCHEMA\")\n",
    "\n",
    "DB_URL = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "# garante schema\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\"))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging - Bronze - Dados Brutos tipos indefinidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela useall.staging_estoque_diario substituída via COPY.\n"
     ]
    }
   ],
   "source": [
    "# staging custos - Loop por ID \n",
    "import io\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "parquet_file = \"data/staging_estoque_day_raw.parquet\"\n",
    "table_name = \"staging_estoque_diario\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "if not df.empty:\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(\n",
    "        csv_buffer,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep=\",\",\n",
    "        quotechar='\"',\n",
    "        quoting=1,  # csv.QUOTE_ALL\n",
    "        escapechar=\"\\\\\",\n",
    "    )\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    cols_with_types = \", \".join([f'\"{col}\" TEXT' for col in df.columns])\n",
    "\n",
    "    # garante tabela + limpa dados (FAST)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {SCHEMA}.{table_name}\"))\n",
    "        conn.execute(\n",
    "            text(\n",
    "                f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{table_name} (\n",
    "                {cols_with_types}\n",
    "            )\n",
    "        \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # COPY ultra-rápido\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "    cursor.copy_expert(\n",
    "        f\"\"\"\n",
    "        COPY {SCHEMA}.{table_name}\n",
    "        FROM STDIN\n",
    "        WITH (\n",
    "            FORMAT CSV,\n",
    "            QUOTE '\"',\n",
    "            ESCAPE '\\\\'\n",
    "        )\n",
    "        \"\"\",\n",
    "        csv_buffer,\n",
    "    )\n",
    "    raw_conn.commit()\n",
    "    cursor.close()\n",
    "    raw_conn.close()\n",
    "\n",
    "    print(f\"Tabela {SCHEMA}.{table_name} substituída via COPY.\")\n",
    "else:\n",
    "    print(\"Parquet vazio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging custos - Loop por ID \n",
    "import io\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "parquet_file = \"data/staging_custos_raw.parquet\"\n",
    "table_name = \"staging_custos\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file, engine=\"pyarrow\")\n",
    "\n",
    "if not df.empty:\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(\n",
    "        csv_buffer,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep=\",\",\n",
    "        quotechar='\"',\n",
    "        quoting=1,  # csv.QUOTE_ALL\n",
    "        escapechar=\"\\\\\",\n",
    "    )\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    cols_with_types = \", \".join([f'\"{col}\" TEXT' for col in df.columns])\n",
    "\n",
    "    # garante tabela + limpa dados (FAST)\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {SCHEMA}.{table_name}\"))\n",
    "        conn.execute(\n",
    "            text(\n",
    "                f\"\"\"\n",
    "            CREATE TABLE {SCHEMA}.{table_name} (\n",
    "                {cols_with_types}\n",
    "            )\n",
    "        \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # COPY ultra-rápido\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "    cursor.copy_expert(\n",
    "        f\"\"\"\n",
    "        COPY {SCHEMA}.{table_name}\n",
    "        FROM STDIN\n",
    "        WITH (\n",
    "            FORMAT CSV,\n",
    "            QUOTE '\"',\n",
    "            ESCAPE '\\\\'\n",
    "        )\n",
    "        \"\"\",\n",
    "        csv_buffer,\n",
    "    )\n",
    "    raw_conn.commit()\n",
    "    cursor.close()\n",
    "    raw_conn.close()\n",
    "\n",
    "    print(f\"Tabela {SCHEMA}.{table_name} substituída via COPY.\")\n",
    "else:\n",
    "    print(\"Parquet vazio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_staging = [\n",
    "    # simples iniciais\n",
    "    \"dfuseallitens\",\n",
    "    \"dfuseallunidades\",\n",
    "    \"dfuseallsegmentos\",\n",
    "    \"dfuseallcidades\",\n",
    "    # complexas no meio\n",
    "    \"dfuseallrequisicoes\",\n",
    "    \"dfuseallestoque\",\n",
    "    \"dfuseallatendimentodereq\",\n",
    "    # simples finais\n",
    "    \"dfuseallsolcompra\",\n",
    "    \"dfuseallfiliais\",\n",
    "    \"dfuseallempresas\",\n",
    "    \"dfuseallexpedição\",\n",
    "    \"dfuseallclientesfornecedore\",\n",
    "    \"dfuseallalmoxarifados\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "\n",
    "def copy_df_to_postgres(df, schema: str, table: str):\n",
    "    import psycopg2\n",
    "    import io\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(buffer, index=False, header=False, sep=\"\\t\", na_rep=\"\\\\N\")\n",
    "    buffer.seek(0)\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=os.getenv(\"PG_DBNAME\"),\n",
    "        user=os.getenv(\"PG_USER\"),\n",
    "        password=os.getenv(\"PG_PASSWORD\"),\n",
    "        host=os.getenv(\"PG_HOST\"),\n",
    "        port=os.getenv(\"PG_PORT\"),\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        COPY {schema}.{table}\n",
    "        FROM STDIN\n",
    "        WITH (FORMAT CSV, DELIMITER E'\\t', NULL '\\\\N')\n",
    "    \"\"\"\n",
    "\n",
    "    cur.copy_expert(sql, buffer)\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "tabelas_criadas = 0\n",
    "dfs_nao_encontrados = []\n",
    "\n",
    "log(\"INICIANDO CARGA STAGING (COPY FROM)\")\n",
    "\n",
    "for df_nome in ordem_staging:\n",
    "    df = globals().get(df_nome)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        dfs_nao_encontrados.append(df_nome)\n",
    "        continue\n",
    "\n",
    "    tabela = \"staging_\" + df_nome.replace(\"dfuseall\", \"\")\n",
    "    tabela = tabela.lower()\n",
    "\n",
    "    log(f\"Preparando tabela {SCHEMA}.{tabela} | Linhas: {len(df)}\")\n",
    "\n",
    "    # 1️⃣ cria estrutura (DDL leve)\n",
    "    with engine.connect() as conn:\n",
    "        df.head(0).to_sql(\n",
    "            name=tabela, con=conn, schema=SCHEMA, if_exists=\"replace\", index=False\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "    log(f\"Iniciando COPY para {SCHEMA}.{tabela}\")\n",
    "\n",
    "    # 2️⃣ carga pesada via COPY\n",
    "    copy_df_to_postgres(df, SCHEMA, tabela)\n",
    "\n",
    "    log(f\"[OK] Tabela {SCHEMA}.{tabela} carregada com sucesso\")\n",
    "\n",
    "    tabelas_criadas += 1\n",
    "\n",
    "\n",
    "# ---------------- FINAL ----------------\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "\n",
    "if tabelas_criadas == 0:\n",
    "    log(\"Nenhuma tabela staging foi criada.\")\n",
    "    log(\"DataFrames não encontrados:\")\n",
    "    for nome in dfs_nao_encontrados:\n",
    "        log(f\" - {nome}\")\n",
    "else:\n",
    "    log(f\"{tabelas_criadas} tabelas staging criadas com sucesso.\")\n",
    "\n",
    "log(\"PROCESSO FINALIZADO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver definindo tipos automaticamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:38] [SCHEMA] schema_silver.json encontrado. Usando e validando tabelas novas.\n",
      "[11:46:38] [SCHEMA] Nova tabela detectada: useall.staging_estoque_diario\n",
      "[11:46:38] [SCHEMA] Inferindo estrutura para useall.silver_estoque_diario\n",
      "[11:46:44] [SCHEMA] schema_silver.json atualizado com novas tabelas\n",
      "[11:46:44] Criando tabela silver useall.silver_custos\n",
      "[11:46:44] Criando tabela silver useall.silver_atendimentodereq\n",
      "[11:46:44] Criando tabela silver useall.silver_itens\n",
      "[11:46:44] Criando tabela silver useall.silver_unidades\n",
      "[11:46:44] Criando tabela silver useall.silver_cidades\n",
      "[11:46:44] Criando tabela silver useall.silver_requisicoes\n",
      "[11:46:44] Criando tabela silver useall.silver_estoque\n",
      "[11:46:44] Criando tabela silver useall.silver_solcompra\n",
      "[11:46:44] Criando tabela silver useall.silver_filiais\n",
      "[11:46:44] Criando tabela silver useall.silver_empresas\n",
      "[11:46:44] Criando tabela silver useall.silver_expedição\n",
      "[11:46:44] Criando tabela silver useall.silver_clientesfornecedore\n",
      "[11:46:44] Criando tabela silver useall.silver_almoxarifados\n",
      "[11:46:44] Criando tabela silver useall.silver_estoque_diario\n",
      "[11:46:44] Carregando dados em useall.silver_custos\n",
      "[11:46:51] [OK] useall.silver_custos carregada\n",
      "[11:46:51] Carregando dados em useall.silver_atendimentodereq\n",
      "[11:47:09] [OK] useall.silver_atendimentodereq carregada\n",
      "[11:47:09] Carregando dados em useall.silver_itens\n",
      "[11:47:09] [OK] useall.silver_itens carregada\n",
      "[11:47:09] Carregando dados em useall.silver_unidades\n",
      "[11:47:09] [OK] useall.silver_unidades carregada\n",
      "[11:47:09] Carregando dados em useall.silver_cidades\n",
      "[11:47:09] [OK] useall.silver_cidades carregada\n",
      "[11:47:09] Carregando dados em useall.silver_requisicoes\n",
      "[11:47:30] [OK] useall.silver_requisicoes carregada\n",
      "[11:47:30] Carregando dados em useall.silver_estoque\n",
      "[11:47:33] [OK] useall.silver_estoque carregada\n",
      "[11:47:33] Carregando dados em useall.silver_solcompra\n",
      "[11:47:33] [OK] useall.silver_solcompra carregada\n",
      "[11:47:33] Carregando dados em useall.silver_filiais\n",
      "[11:47:33] [OK] useall.silver_filiais carregada\n",
      "[11:47:33] Carregando dados em useall.silver_empresas\n",
      "[11:47:33] [OK] useall.silver_empresas carregada\n",
      "[11:47:33] Carregando dados em useall.silver_expedição\n",
      "[11:47:34] [OK] useall.silver_expedição carregada\n",
      "[11:47:34] Carregando dados em useall.silver_clientesfornecedore\n",
      "[11:47:34] [OK] useall.silver_clientesfornecedore carregada\n",
      "[11:47:34] Carregando dados em useall.silver_almoxarifados\n",
      "[11:47:34] [OK] useall.silver_almoxarifados carregada\n",
      "[11:47:34] Carregando dados em useall.silver_estoque_diario\n",
      "[11:47:53] [OK] useall.silver_estoque_diario carregada\n",
      "[11:47:53] --------------------------------------------------\n",
      "[11:47:53] PROCESSO FINALIZADO\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SCHEMA_FILE = Path(\"schema_silver.json\")\n",
    "\n",
    "SCHEMA = \"useall\"\n",
    "SAMPLE_LIMIT = 100000\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "def looks_like_text(s: pd.Series):\n",
    "    sample = s.dropna().astype(str).head(100)\n",
    "    return sample.str.contains(r\"[A-Za-zÇç]\").any()\n",
    "\n",
    "\n",
    "def load_or_create_schema(engine, schema, staging_tables):\n",
    "    # Carrega schema existente ou inicia vazio\n",
    "    if SCHEMA_FILE.exists():\n",
    "        log(\"[SCHEMA] schema_silver.json encontrado. Usando e validando tabelas novas.\")\n",
    "        with open(SCHEMA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            schema_silver = json.load(f)\n",
    "    else:\n",
    "        log(\"[SCHEMA] schema_silver.json não encontrado. Criando do zero.\")\n",
    "        schema_silver = {}\n",
    "\n",
    "    updated = False\n",
    "\n",
    "    for staging_table in staging_tables:\n",
    "        silver_table = silver_table_name(staging_table)\n",
    "\n",
    "        # Já existe → não toca (schema congelado)\n",
    "        if silver_table in schema_silver:\n",
    "            continue\n",
    "\n",
    "        log(f\"[SCHEMA] Nova tabela detectada: {schema}.{staging_table}\")\n",
    "        log(f\"[SCHEMA] Inferindo estrutura para {schema}.{silver_table}\")\n",
    "\n",
    "        df_sample = pd.read_sql(\n",
    "            f'SELECT * FROM {schema}.\"{staging_table}\" LIMIT {SAMPLE_LIMIT}',\n",
    "            engine\n",
    "        )\n",
    "\n",
    "        schema_silver[silver_table] = {\n",
    "            \"staging_table\": staging_table,\n",
    "            \"columns\": {\n",
    "                col.lower(): {\n",
    "                    **infer_column_type_final(df_sample[col]),\n",
    "                    \"source_col\": col,\n",
    "                }\n",
    "                for col in df_sample.columns\n",
    "            },\n",
    "        }\n",
    "\n",
    "        updated = True\n",
    "\n",
    "    # Salva somente se houve mudança\n",
    "    if updated:\n",
    "        with open(SCHEMA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(schema_silver, f, indent=2, ensure_ascii=False)\n",
    "        log(\"[SCHEMA] schema_silver.json atualizado com novas tabelas\")\n",
    "    else:\n",
    "        log(\"[SCHEMA] Nenhuma nova tabela detectada\")\n",
    "\n",
    "    return schema_silver\n",
    "\n",
    "# Detecta formato de data\n",
    "def is_date_series(s: pd.Series, threshold=0.9):\n",
    "    sample = s.dropna().astype(str).head(100)\n",
    "    if sample.empty:\n",
    "        return None\n",
    "\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d/%m/%Y %H:%M:%S\",\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        ok = 0\n",
    "        for v in sample:\n",
    "            try:\n",
    "                pd.to_datetime(v, format=fmt)\n",
    "                ok += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if ok / len(sample) >= threshold:\n",
    "            return fmt\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Inferência de tipo\n",
    "def infer_column_type_final(series: pd.Series) -> dict:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return {\"type\": \"text\"}\n",
    "\n",
    "    # BOOLEAN lógico\n",
    "    if s.astype(str).isin([\"0\", \"1\", \"true\", \"false\", \"True\", \"False\"]).all():\n",
    "        return {\"type\": \"boolean\"}\n",
    "\n",
    "    # DATE / TIMESTAMP (somente se NÃO for texto)\n",
    "    if not looks_like_text(s):\n",
    "        date_fmt = is_date_series(s)\n",
    "        if date_fmt:\n",
    "            return {\"type\": \"timestamp\", \"format\": date_fmt}\n",
    "\n",
    "    # INTEGER\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+\").all():\n",
    "        return {\"type\": \"bigint\"}\n",
    "\n",
    "    # DECIMAL\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+(\\.\\d+)?\").all():\n",
    "        return {\"type\": \"numeric(18,4)\"}\n",
    "\n",
    "    return {\"type\": \"text\"}\n",
    "\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(\n",
    "        text(\n",
    "            \"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = :schema\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE 'staging_%'\n",
    "    \"\"\"\n",
    "        ),\n",
    "        {\"schema\": SCHEMA},\n",
    "    )\n",
    "\n",
    "    staging_tables = [row[0] for row in result.fetchall()]\n",
    "\n",
    "\n",
    "# Função nome silver\n",
    "def silver_table_name(staging_table: str) -> str:\n",
    "    return staging_table.replace(\"staging_\", \"silver_\")\n",
    "\n",
    "\n",
    "schema_silver = load_or_create_schema(\n",
    "    engine=engine, schema=SCHEMA, staging_tables=staging_tables\n",
    ")\n",
    "\n",
    "\n",
    "# Cria cast SQL\n",
    "def generate_cast_sql(col_dest, meta):\n",
    "    col_src = meta[\"source_col\"]\n",
    "\n",
    "    col_sql = f'\"{col_src}\"'\n",
    "    col_txt = f\"{col_sql}::text\"\n",
    "\n",
    "    if meta[\"type\"] == \"boolean\":\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN lower({col_txt}) IN ('1','true','sim','s','y','yes') THEN true\n",
    "            WHEN lower({col_txt}) IN ('0','false','nao','n','no') THEN false\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    if meta[\"type\"] == \"timestamp\":\n",
    "        fmt = meta.get(\"format\")\n",
    "\n",
    "        pg_fmt_map = {\n",
    "            \"%Y-%m-%d\": \"YYYY-MM-DD\",\n",
    "            \"%Y-%m-%d %H:%M:%S\": \"YYYY-MM-DD HH24:MI:SS\",\n",
    "            \"%Y-%m-%dT%H:%M:%S\": 'YYYY-MM-DD\"T\"HH24:MI:SS',\n",
    "            \"%d/%m/%Y\": \"DD/MM/YYYY\",\n",
    "            \"%d/%m/%Y %H:%M:%S\": \"DD/MM/YYYY HH24:MI:SS\",\n",
    "        }\n",
    "\n",
    "        pg_fmt = pg_fmt_map.get(fmt)\n",
    "\n",
    "        if pg_fmt:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_txt} ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}' THEN\n",
    "                    to_timestamp({col_txt}, '{pg_fmt}')\n",
    "                ELSE NULL\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_txt} = '' THEN NULL\n",
    "                ELSE NULL\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "    if meta[\"type\"] in (\"bigint\", \"numeric(18,4)\"):\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN {col_txt} ~ '^-?\\\\d+(\\\\.\\\\d+)?$' THEN {col_txt}::{meta[\"type\"]}\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    return f'{col_sql}::text AS \"{col_dest}\"'\n",
    "\n",
    "\n",
    "# Gera CREATE TABLE\n",
    "def generate_create_table(schema, table, columns: dict):\n",
    "    cols = \",\\n  \".join(\n",
    "        f'\"{col_dest}\" {meta[\"type\"]}' for col_dest, meta in columns.items()\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    DROP TABLE IF EXISTS {schema}.\"{table}\";\n",
    "    CREATE TABLE {schema}.\"{table}\" (\n",
    "      {cols}\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Cria tabelas silver\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    log(f\"Criando tabela silver {SCHEMA}.{silver_table}\")\n",
    "    ddl = generate_create_table(SCHEMA, silver_table, meta[\"columns\"])\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl))\n",
    "\n",
    "\n",
    "# Gera INSERT\n",
    "def generate_insert_cast(staging_schema, SCHEMA, staging_table, silver_table, columns):\n",
    "    selects = \",\\n\".join(\n",
    "        generate_cast_sql(col_dest, meta) for col_dest, meta in columns.items()\n",
    "    )\n",
    "    return f\"\"\"\n",
    "    INSERT INTO {SCHEMA}.\"{silver_table}\"\n",
    "    SELECT\n",
    "      {selects}\n",
    "    FROM {staging_schema}.\"{staging_table}\";\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Carrega dados\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    staging_table = meta[\"staging_table\"]\n",
    "    columns = meta[\"columns\"]\n",
    "    log(f\"Carregando dados em {SCHEMA}.{silver_table}\")\n",
    "    sql = generate_insert_cast(SCHEMA, SCHEMA, staging_table, silver_table, columns)\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        log(f\"[OK] {SCHEMA}.{silver_table} carregada\")\n",
    "    except Exception as e:\n",
    "        log(f\"[ERRO] {SCHEMA}.{silver_table} -> {e}\")\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "log(\"PROCESSO FINALIZADO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold - Adicionando novas colunas e agregando valor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "DO $$\n",
    "DECLARE\n",
    "    r RECORD;\n",
    "    gold_table TEXT;\n",
    "BEGIN\n",
    "    FOR r IN\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'useall'\n",
    "          AND table_name LIKE 'silver_%'\n",
    "    LOOP\n",
    "\n",
    "        gold_table := replace(r.table_name, 'silver_', 'gold_');\n",
    "\n",
    "        -- CASO ESPECIAL: SILVER_REQUISICOES\n",
    "        IF r.table_name = 'silver_requisicoes' THEN\n",
    "\n",
    "            -- cria se não existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes\n",
    "                 WHERE false;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "        -- DEMAIS TABELAS\n",
    "        ELSE\n",
    "\n",
    "            -- cria se não existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT * FROM useall.%I WHERE false;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT * FROM useall.%I;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "        END IF;\n",
    "\n",
    "    END LOOP;\n",
    "END $$;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de estoque\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_ultimos_custos AS\n",
    "\n",
    "SELECT\n",
    "    *\n",
    "FROM (\n",
    "    SELECT\n",
    "        gc.*,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY codigoitem\n",
    "            ORDER BY datacusto DESC\n",
    "        ) AS rn\n",
    "    FROM useall.gold_custos gc\n",
    "    WHERE ultimocusto <> 0\n",
    ") t\n",
    "WHERE rn = 1;\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de estoque\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_gold_estoque AS\n",
    "SELECT\n",
    "    e.*,\n",
    "    c.ultimocusto,\n",
    "\n",
    "    /* =========================\n",
    "       STATUS DO ESTOQUE\n",
    "       ========================= */\n",
    "    CASE\n",
    "        WHEN e.estoquemin IS NOT NULL\n",
    "             AND e.estoquemax IS NOT NULL\n",
    "             AND e.estoquemin > e.estoquemax\n",
    "        THEN 'PARAMETRO_INVALIDO'\n",
    "\n",
    "        WHEN e.saldodisponivel < 0\n",
    "        THEN 'INCONSISTENTE'\n",
    "\n",
    "        WHEN e.estoquemin IS NULL OR e.estoquemin = 0\n",
    "        THEN 'SEM_MINIMO'\n",
    "\n",
    "        WHEN e.estoquemin > 0\n",
    "             AND e.saldodisponivel = 0\n",
    "        THEN 'RUPTURA'\n",
    "\n",
    "        WHEN e.estoquemin > 0\n",
    "             AND e.saldodisponivel > 0\n",
    "             AND e.saldodisponivel < e.estoquemin\n",
    "        THEN 'CRITICO'\n",
    "\n",
    "        WHEN e.estoquemax IS NOT NULL\n",
    "             AND e.saldodisponivel > e.estoquemax\n",
    "        THEN 'EXCESSO'\n",
    "\n",
    "        WHEN e.saldodisponivel >= e.estoquemin\n",
    "             AND (e.estoquemax IS NULL OR e.saldodisponivel <= e.estoquemax)\n",
    "        THEN 'ADEQUADO'\n",
    "\n",
    "        ELSE 'NAO_CLASSIFICADO'\n",
    "    END AS py_status_estoque,\n",
    "\n",
    "    /* =========================\n",
    "       IMPACTO FINANCEIRO\n",
    "       + = reposição\n",
    "       - = excesso\n",
    "       ========================= */\n",
    "    CASE\n",
    "        -- erro de parâmetro → neutraliza\n",
    "        WHEN e.estoquemin IS NOT NULL\n",
    "             AND e.estoquemax IS NOT NULL\n",
    "             AND e.estoquemin > e.estoquemax\n",
    "        THEN 0\n",
    "\n",
    "        -- sem mínimo → neutro\n",
    "        WHEN e.estoquemin IS NULL OR e.estoquemin = 0\n",
    "        THEN 0\n",
    "\n",
    "        -- reposição necessária\n",
    "        WHEN e.saldodisponivel < e.estoquemin\n",
    "        THEN (e.estoquemin - GREATEST(e.saldodisponivel, 0)) * c.ultimocusto\n",
    "\n",
    "        -- excesso de estoque (valor negativo)\n",
    "        WHEN e.estoquemax IS NOT NULL\n",
    "             AND e.saldodisponivel > e.estoquemax\n",
    "        THEN -1 * (e.saldodisponivel - e.estoquemax) * c.ultimocusto\n",
    "\n",
    "        ELSE 0\n",
    "    END AS valor_impacto_estoque\n",
    "\n",
    "FROM useall.gold_estoque e\n",
    "LEFT JOIN useall.vw_ultimos_custos c\n",
    "       ON c.codigoitem = e.iditem;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de estoque\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_gold_filiais_uf AS\n",
    "\n",
    "SELECT \n",
    "    idfilial,\n",
    "    datahoraalt,\n",
    "    idempresa,\n",
    "    matriz,\n",
    "    apelido,\n",
    "    CASE \n",
    "        -- Rio Grande do Sul\n",
    "        WHEN idfilial IN (393, 336, 337, 558, 387) THEN 'RS'\n",
    "        \n",
    "        -- Bahia\n",
    "        WHEN idfilial = 520 THEN 'BA'\n",
    "        \n",
    "        -- Distrito Federal\n",
    "        WHEN idfilial = 404 THEN 'DF'\n",
    "        \n",
    "        -- Santa Catarina (Mapeamento explícito dos IDs atuais)\n",
    "        WHEN idfilial IN (342, 343, 381, 389, 334, 335, 339, 333, 341, 578, 390, 379, 344, 345, 346, 338) THEN 'SC'\n",
    "        \n",
    "        -- Caso surja um ID novo que não foi tratado acima\n",
    "        ELSE '*NOVA'\n",
    "    END AS uf\n",
    "FROM useall.gold_filiais;\n",
    "\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de requisicoes\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_gold_requisicoes_status AS\n",
    "\n",
    "SELECT\n",
    "    r.*,\n",
    "\n",
    "    /* STATUS DO ITEM */\n",
    "    CASE\n",
    "        WHEN r.quantcancel = r.quant THEN 'CANCELADO TOTAL'\n",
    "        WHEN r.quantsubst = r.quant THEN 'SUBSTITUIDO TOTAL'\n",
    "        WHEN r.quantatend = 0 AND r.saldo > 0 THEN 'NÃO ATENDIDA'\n",
    "        WHEN r.quantatend = r.quant THEN 'ATENDIDO'\n",
    "        WHEN r.quantatend > r.quant THEN 'ATENDIDO A MAIS'\n",
    "        WHEN r.quantatend < r.quant AND r.quantatend > 0 THEN 'ATENDIDA PARCIAL'\n",
    "        ELSE 'INDEFINIDO'\n",
    "    END AS py_status_item,\n",
    "\n",
    "    /* GERA ATENDIMENTO */\n",
    "    CASE\n",
    "        WHEN r.quantatend > 0 THEN 'SIM'\n",
    "        ELSE 'NÃO'\n",
    "    END AS py_gera_atend,\n",
    "\n",
    "    /* ID COMPOSTO */\n",
    "    r.idreqmat::text || '-' || r.iditem::text AS py_idreqitem,\n",
    "\n",
    "    /* MAIOR DATA DE ATENDIMENTO */\n",
    "    COALESCE(\n",
    "        ati.max_dataatend_item,\n",
    "        atr.max_dataatend_req\n",
    "    ) AS py_data_ult_atend\n",
    "\n",
    "FROM useall.gold_requisicoes r\n",
    "\n",
    "/* ATENDIMENTO POR ITEM */\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        py_idreqitem,\n",
    "        MAX(data_atend) AS max_dataatend_item\n",
    "    FROM useall.vw_gold_atendimentos_id\n",
    "    GROUP BY py_idreqitem\n",
    ") ati\n",
    "    ON ati.py_idreqitem = (r.idreqmat::text || '-' || r.iditem::text)\n",
    "\n",
    "/* ATENDIMENTO POR REQUISIÇÃO (fallback) */\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        idreqmat,\n",
    "        MAX(data_atend) AS max_dataatend_req\n",
    "    FROM useall.vw_gold_atendimentos_id\n",
    "    GROUP BY idreqmat\n",
    ") atr\n",
    "    ON atr.idreqmat = r.idreqmat;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de requisicoes\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_gold_atendimentos_id AS\n",
    "SELECT\n",
    "    a.*,\n",
    "\n",
    "    /* ID REQUISIÇÃO + ITEM */\n",
    "    a.idreqmat::text || '-' || a.iditem::text AS py_idreqitem,\n",
    "\n",
    "    /* ID ITEM + DATA */\n",
    "    a.iditem::text || '-' || TO_CHAR(a.data_atend::date, 'YYYYMMDD') AS py_iddataitem\n",
    "\n",
    "FROM useall.gold_atendimentodereq a;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar view de requisicoes\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW useall.vw_gold_estoque_dia_matriz_id AS\n",
    "SELECT\n",
    "    *,\n",
    "    CONCAT(\n",
    "        iditem,\n",
    "        '-',\n",
    "        TO_CHAR(data_referencia::date, 'YYYYMMDD')\n",
    "    ) AS py_iddataitem\n",
    "FROM useall.gold_estoque_diario\n",
    "WHERE desc_almox = 'MERC. MATRIZ';\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim_Calendario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ---------------- SQL ----------------\n",
    "sql_create_dim_calendario = text(\n",
    "    \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS useall.dim_calendario (\n",
    "    data DATE PRIMARY KEY,\n",
    "\n",
    "    ano INT,\n",
    "    mes INT,\n",
    "    dia INT,\n",
    "\n",
    "    ano_mes TEXT,\n",
    "    ano_mes_atual TEXT,\n",
    "    ano_mes_ordem INT,\n",
    "\n",
    "    nome_mes TEXT,\n",
    "    nome_mes_abrev TEXT,\n",
    "    nome_dia TEXT,\n",
    "    nome_dia_abrev TEXT,\n",
    "\n",
    "    dia_semana INT,\n",
    "    semana_iso INT,\n",
    "    ano_iso INT,\n",
    "    trimestre INT,\n",
    "    \n",
    "    dia_mes_abr TEXT,\n",
    "\n",
    "    is_fim_de_semana BOOLEAN,\n",
    "    is_feriado BOOLEAN,\n",
    "    nome_feriado TEXT\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "sql_create_indices = text(\n",
    "    \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_data\n",
    "    ON useall.dim_calendario (data);\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_ano_mes_ordem\n",
    "    ON useall.dim_calendario (ano_mes_ordem);\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "sql_atualiza_calendario = text(\n",
    "    \"\"\"\n",
    "INSERT INTO useall.dim_calendario (\n",
    "    data,\n",
    "    ano,\n",
    "    mes,\n",
    "    dia,\n",
    "    ano_mes,\n",
    "    ano_mes_atual,\n",
    "    ano_mes_ordem,\n",
    "    nome_mes,\n",
    "    nome_mes_abrev,\n",
    "    nome_dia,\n",
    "    nome_dia_abrev,\n",
    "    dia_semana,\n",
    "    semana_iso,\n",
    "    ano_iso,\n",
    "    trimestre,\n",
    "    dia_mes_abr,\n",
    "    is_fim_de_semana,\n",
    "    is_feriado,\n",
    "    nome_feriado\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    d::date AS data,\n",
    "\n",
    "    EXTRACT(YEAR FROM d)::int AS ano,\n",
    "    EXTRACT(MONTH FROM d)::int AS mes,\n",
    "    EXTRACT(DAY FROM d)::int AS dia,\n",
    "\n",
    "    TO_CHAR(d, 'YYYY/MM') AS ano_mes,\n",
    "\n",
    "    CASE\n",
    "        WHEN EXTRACT(YEAR FROM d) = EXTRACT(YEAR FROM CURRENT_DATE)\n",
    "         AND EXTRACT(MONTH FROM d) = EXTRACT(MONTH FROM CURRENT_DATE)\n",
    "        THEN 'Mês Atual'\n",
    "        ELSE TO_CHAR(d, 'YYYY/MM')\n",
    "    END AS ano_mes_atual,\n",
    "\n",
    "    (EXTRACT(YEAR FROM d) * 100 + EXTRACT(MONTH FROM d))::int AS ano_mes_ordem,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Janeiro'\n",
    "        WHEN 2 THEN 'Fevereiro'\n",
    "        WHEN 3 THEN 'Março'\n",
    "        WHEN 4 THEN 'Abril'\n",
    "        WHEN 5 THEN 'Maio'\n",
    "        WHEN 6 THEN 'Junho'\n",
    "        WHEN 7 THEN 'Julho'\n",
    "        WHEN 8 THEN 'Agosto'\n",
    "        WHEN 9 THEN 'Setembro'\n",
    "        WHEN 10 THEN 'Outubro'\n",
    "        WHEN 11 THEN 'Novembro'\n",
    "        WHEN 12 THEN 'Dezembro'\n",
    "    END AS nome_mes,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Jan'\n",
    "        WHEN 2 THEN 'Fev'\n",
    "        WHEN 3 THEN 'Mar'\n",
    "        WHEN 4 THEN 'Abr'\n",
    "        WHEN 5 THEN 'Mai'\n",
    "        WHEN 6 THEN 'Jun'\n",
    "        WHEN 7 THEN 'Jul'\n",
    "        WHEN 8 THEN 'Ago'\n",
    "        WHEN 9 THEN 'Set'\n",
    "        WHEN 10 THEN 'Out'\n",
    "        WHEN 11 THEN 'Nov'\n",
    "        WHEN 12 THEN 'Dez'\n",
    "    END AS nome_mes_abrev,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Segunda-feira'\n",
    "        WHEN 2 THEN 'Terça-feira'\n",
    "        WHEN 3 THEN 'Quarta-feira'\n",
    "        WHEN 4 THEN 'Quinta-feira'\n",
    "        WHEN 5 THEN 'Sexta-feira'\n",
    "        WHEN 6 THEN 'Sábado'\n",
    "        WHEN 7 THEN 'Domingo'\n",
    "    END AS nome_dia,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Seg'\n",
    "        WHEN 2 THEN 'Ter'\n",
    "        WHEN 3 THEN 'Qua'\n",
    "        WHEN 4 THEN 'Qui'\n",
    "        WHEN 5 THEN 'Sex'\n",
    "        WHEN 6 THEN 'Sáb'\n",
    "        WHEN 7 THEN 'Dom'\n",
    "    END AS nome_dia_abrev,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d)::int AS dia_semana,\n",
    "    EXTRACT(WEEK FROM d)::int AS semana_iso,\n",
    "    EXTRACT(ISOYEAR FROM d)::int AS ano_iso,\n",
    "    EXTRACT(QUARTER FROM d)::int AS trimestre,\n",
    "\n",
    "    TO_CHAR(d, 'DD/MM') AS dia_mes_abr,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d) IN (6,7) AS is_fim_de_semana,\n",
    "    FALSE AS is_feriado,\n",
    "    NULL AS nome_feriado\n",
    "FROM (\n",
    "    SELECT DISTINCT data::date AS d\n",
    "    FROM useall.gold_requisicoes\n",
    "    WHERE data IS NOT NULL\n",
    ") x\n",
    "ON CONFLICT (data) DO NOTHING;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ---------------- EXECUÇÃO ----------------\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql_create_dim_calendario)\n",
    "    conn.execute(sql_create_indices)\n",
    "    conn.execute(sql_atualiza_calendario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
