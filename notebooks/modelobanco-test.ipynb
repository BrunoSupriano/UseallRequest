{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraÃ§Ã£o de API Useall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline iniciada em 14/01/2026 17:58:05 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "# --- ConfiguraÃ§Ãµes ---\n",
    "BASE_URL = \"https://extracao.useallcloud.com.br/api/v1/json/\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"use-relatorio-token\": \"eyJJZCI6ImNkMDNkMzhiLWZhNzUtNDg4Yi04NDA1LTg5OTU1MzBjNjFiMSIsIlN0cmluZ0NvbmV4YW8iOiJBeHh4IFh2a2VqST10bGRxZGZ4Rzk7c3FJMSBCbz1ScUl4WHlnYWVoO2dNcXp6djFvPXNxSXVleGd5SW5TSSQkOyIsIkNvZGlnb1VzdWFyaW8iOjczMzIsIkNvZGlnb1RlbmFudCI6MTQzfQ==\"\n",
    "}\n",
    "\n",
    "def buscar_dados_api(identificacao, nome_arquivo, backend_filters=None, extra_params=None):\n",
    "    \"\"\"Busca dados na API UseAll e retorna um DataFrame (ou None em caso de erro/vazio)\"\"\"\n",
    "    \n",
    "    query_params = {\"Identificacao\": identificacao}\n",
    "    \n",
    "    if backend_filters:\n",
    "        query_params[\"FiltrosSqlQuery\"] = json.dumps(backend_filters, ensure_ascii=False)\n",
    "        \n",
    "    if extra_params:\n",
    "        query_params.update(extra_params)\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Iniciando extraÃ§Ã£o: {nome_arquivo}...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, headers=HEADERS, params=query_params, timeout=500)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Erro 429 (Too Many Requests) em {nome_arquivo}. Aguardando 185 segundos...\")\n",
    "                time.sleep(185)\n",
    "                continue\n",
    "                \n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            return df\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Timeout atingido para {nome_arquivo}. Aguardando 185 segundos...\")\n",
    "            time.sleep(185)\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Erro irrecuperÃ¡vel em {nome_arquivo}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def salvar_parquet(df, nome_arquivo):\n",
    "    \"\"\"Salva o DataFrame em arquivo parquet\"\"\"\n",
    "    if df is not None and not df.empty:\n",
    "        # Garante extensÃ£o .parquet\n",
    "        if not nome_arquivo.endswith('.parquet'):\n",
    "            nome_arquivo += '.parquet'\n",
    "        \n",
    "        try:\n",
    "            df.to_parquet(nome_arquivo, index=False)\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Sucesso ao salvar: {nome_arquivo} (Linhas: {len(df)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Erro ao salvar {nome_arquivo}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Nada a salvar para {nome_arquivo} (DataFrame vazio ou None)\")\n",
    "\n",
    "def verificar_tipos_dados():\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] VERIFICAÃ‡ÃƒO DE TIPOS DE DADOS\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    encontrou = False\n",
    "\n",
    "    for nome, obj in globals().items():\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            encontrou = True\n",
    "            print(f\"\\nDataFrame: {nome}\")\n",
    "            if not obj.empty:\n",
    "                print(\"-\" * 30)\n",
    "                print(obj.dtypes)\n",
    "                print(\"-\" * 30)\n",
    "            else:\n",
    "                print(\"  (DataFrame vazio)\")\n",
    "\n",
    "    if not encontrou:\n",
    "        print(\"Nenhum DataFrame encontrado em memÃ³ria.\")\n",
    "\n",
    "# --- Defines Auxiliares de Filtro ---\n",
    "def filtro_simples(nome, valor):\n",
    "    return {\"Nome\": nome, \"Valor\": valor, \"Operador\": None, \"Descricao\": None, \"ValorFormatado\": None}\n",
    "\n",
    "def carregar_dfs_globais(tarefas):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO CARGA EM MEMÃ“RIA ---\")\n",
    "\n",
    "    for t in tarefas:\n",
    "        nome = t[\"nome\"]\n",
    "        df = buscar_dados_api(\n",
    "            t[\"id\"],\n",
    "            nome,\n",
    "            t.get(\"filtros\"),\n",
    "            t.get(\"extra_params\")\n",
    "        )\n",
    "\n",
    "        if df is not None:\n",
    "            globals()[nome] = df\n",
    "        else:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "def carregar_tarefa_complexa(tarefa):\n",
    "    nome = tarefa[\"nome\"]\n",
    "\n",
    "    df = buscar_dados_api(\n",
    "        tarefa[\"id\"],\n",
    "        nome,\n",
    "        tarefa.get(\"filtros\"),\n",
    "        tarefa.get(\"extra_params\")\n",
    "    )\n",
    "\n",
    "    if df is not None:\n",
    "        globals()[nome] = df\n",
    "    else:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Falha ao carregar {nome}\")\n",
    "\n",
    "\n",
    "pipeline_start = time.time()\n",
    "print(f\"--- Pipeline iniciada em {time.strftime('%d/%m/%Y %H:%M:%S')} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis de filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fixos = {\"pagina\": 1, \"qtderegistros\": 1}\n",
    "\n",
    "tarefas_simples = [\n",
    "    {\n",
    "        \"nome\": \"dfuseallitens\",\n",
    "        \"id\": \"m2_estoque_item\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallunidades\",\n",
    "        \"id\": \"m2_estoque_unidade\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsegmentos\",\n",
    "        \"id\": \"m2_vendas_segmento\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallcidades\",\n",
    "        \"id\": \"m2_geral_cidades\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallsolcompra\",\n",
    "        \"id\": \"m2_compras_m2_compras_solicitacao_de_compras__extra\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DataFim\", \"01/01/2500\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallfiliais\",\n",
    "        \"id\": \"m2_geral_filiais\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/1900, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2500, 14:00:00\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallempresas\",\n",
    "        \"id\": \"m2_geral_empresas\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTINI\", \"01/01/2022, 11:00:00\"),\n",
    "            filtro_simples(\"DATAHORAALTFIM\", \"01/01/2027, 14:00:00\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallexpediÃ§Ã£o\",\n",
    "        \"id\": \"m2_vendas_extracao_de_dados__saida_expedicao\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"data1\", \"01/01/1900\"),\n",
    "            filtro_simples(\"data2\", \"01/01/2500\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"dfuseallclientesfornecedore\",\n",
    "        \"id\": \"m2_geral_clientes__fornecedores\",\n",
    "        \"filtros\": [\n",
    "            filtro_simples(\"DATAHORAALTERACAOINI\", \"01/01/1900\"),\n",
    "            filtro_simples(\"DATAHORAALTERACAOFIM\", \"01/01/2027\")\n",
    "        ],\n",
    "        \"extra_params\": params_fixos\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPLEXAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtros_req = [\n",
    "    {\"Nome\": \"IDFILIAL\", \"Valor\": [333,334,335,336,387,404,520,558,578,339,340,342,343,341,344,345,346,381,389,390], \"Operador\": 1},\n",
    "    {\"Nome\": \"DATA\", \"Valor\": \"01/01/2010,01/01/2027\", \"Operador\": 8, \"TipoPeriodoData\": 5},\n",
    "    {\"Nome\": \"DATAPREVATEND\", \"Valor\": \"01/01/2010,01/01/2027\", \"Operador\": 8, \"TipoPeriodoData\": 8},\n",
    "    {\"Nome\": \"CLASSGRUPOITEM\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"CLASSCONTACDC\", \"Valor\": \"\"},\n",
    "    {\"Nome\": \"quebra\", \"Valor\": 1},\n",
    "    {\"Nome\": \"FILTROSWHERE\", \"Valor\": \" AND IDEMPRESA = 211\"}\n",
    "]\n",
    "\n",
    "filtros_estoque = [\n",
    "    {\"Nome\": \"ADDATA\", \"Valor\": \"08/01/2010\"},\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": \" AND EXISTS (SELECT 1 FROM USE_USUARIOS_FILIAIS UFILIAIS \"\n",
    "                \"WHERE UFILIAIS.IDEMPRESA = T.IDEMPRESA \"\n",
    "                \"AND UFILIAIS.IDFILIAL = T.IDFILIAL \"\n",
    "                \"AND UFILIAIS.IDUSUARIO = 7332) \"\n",
    "                \"AND T.IDFILIAL IN (333,334,336,404,335,387,520,558,578)\"\n",
    "    },\n",
    "    {\"Nome\": \"ANQUEBRA\", \"Valor\": 0}\n",
    "]\n",
    "\n",
    "filtros_atend = [\n",
    "    {\n",
    "        \"Nome\": \"FILTROSWHERE\",\n",
    "        \"Valor\": (\n",
    "            \"WHERE IDEMPRESA = 211 \"\n",
    "            \"AND IDFILIAL IN (333,334,335,336,387,404,520,558,339,578,340,342,343,341,344,345,346,381,389,390) \"\n",
    "            \"AND DATA_REQ BETWEEN '01/01/1900' AND '01/01/2900' \"\n",
    "            \"AND DATA_ATEND BETWEEN '01/01/1900' AND '01/01/2900'\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "params_atend = {\n",
    "    \"NomeOrganizacao\": \"SETUP SERVICOS ESPECIALIZADOS LTDA\",\n",
    "    \"Parametros\": json.dumps([\n",
    "        {\"Nome\": \"usecellmerging\", \"Valor\": True},\n",
    "        {\"Nome\": \"quebra\", \"Valor\": 0}\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 1 â€” REQUISIÃ‡Ã•ES\n",
    "# ===============================\n",
    "\n",
    "tarefa_requisicoes = {\n",
    "    \"nome\": \"dfuseallrequisicoes\",\n",
    "    \"id\": \"m2_estoque_requisicao_de_materiais\",\n",
    "    \"filtros\": filtros_req,\n",
    "    \"extra_params\": None\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 2 â€” ESTOQUE\n",
    "# ===============================\n",
    "\n",
    "tarefa_estoque = {\n",
    "    \"nome\": \"dfuseallestoque\",\n",
    "    \"id\": \"09249662000174_m2_estoque_saldo_de_estoque__setup\",\n",
    "    \"filtros\": filtros_estoque,\n",
    "    \"extra_params\": None\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# BLOCO 3 â€” ATENDIMENTO DE REQUISIÃ‡Ã•ES\n",
    "# ===============================\n",
    "\n",
    "tarefa_atendimento = {\n",
    "    \"nome\": \"dfuseallatendimentodereq\",\n",
    "    \"id\": \"m2_estoque_atendimentos_de_requisicao\",\n",
    "    \"filtros\": filtros_atend,\n",
    "    \"extra_params\": params_atend\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando funÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:05] --- INICIANDO CARGA EM MEMÃ“RIA ---\n",
      "[17:58:05] Iniciando extraÃ§Ã£o: dfuseallitens...\n",
      "[17:58:06] Erro 429 (Too Many Requests) em dfuseallitens. Aguardando 185 segundos...\n"
     ]
    }
   ],
   "source": [
    "carregar_dfs_globais(tarefas_simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carregar_tarefa_complexa(tarefa_estoque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carregar_tarefa_complexa(tarefa_requisicoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ===============================\n",
    "# BLOCO 4 â€” CUSTOS DE ESTOQUE\n",
    "# ===============================\n",
    "\n",
    "filtros_custos = [\n",
    "    {\"Nome\": \"IDFILIAL\", \"Valor\": [333,334,335,336,387,404,520,558,339,340,341,342,343,344,345,346,381,389,390], \"Operador\": 1},\n",
    "    {\"Nome\": \"FILTROSREGISTROSATIVO\", \"Valor\": \"AND IA.ATIVO = 1 AND I.ATIVO = 1\"},\n",
    "    {\"Nome\": \"DATA\", \"Valor\": \"13/01/2026\"}\n",
    "]\n",
    "\n",
    "params_custos = {\n",
    "    \"Parametros\": [\n",
    "        {\"Nome\": \"usecellmerging\", \"Valor\": True},\n",
    "        {\"Nome\": \"filter\", \"Valor\": (\n",
    "            \"Filiais = SETUP AUTOMACAO E SEGURANCA, LOJA - ARARANGUA, LOJA - CRICIUMA, \"\n",
    "            \"SETUP PELOTAS, SETUP BAHIA, SETUP ELDORADO DO SUL, SETUP BRASILIA, \"\n",
    "            \"SETUP OS\\u00d3RIO, VIGILANCIA SETUP, SETUP FLORIAN\\u00d3POLIS, \"\n",
    "            \"PINHEIRINHO SERVI\\u00c7OS, SETUP COMERCIO MATRIZ, CTFM - ILLUMINATIO ARARANGUA, \"\n",
    "            \"CTFM - ILLUMINATIO CD, VM - DISTRIBUIDORA ARARANGUA MATRIZ, VM - DISTRIBUIDORA CD, \"\n",
    "            \"ENGECO PROJETOS E CONSTRUCOES LTDA, VM - DISTRIBUIDORA CRICIUMA, FFW ADMINISTRADORA DE BENS, \"\n",
    "            \"SETUP LOCA\\u00c7\\u00d5ES; Data = 13/01/2026; Custo = Ambos; \"\n",
    "            \"Apenas itens com saldo = N\\u00e3o; Apenas itens e almoxarifados ativos = Sim\"\n",
    "        )}\n",
    "    ]\n",
    "}\n",
    "\n",
    "tarefa_custos = {\n",
    "    \"nome\": \"dfuseallcustosestoque\",\n",
    "    \"id\": \"m2_estoque_custos\",\n",
    "    \"filtros\": filtros_custos,\n",
    "    \"extra_params\": params_custos\n",
    "}\n",
    "\n",
    "carregar_tarefa_complexa(tarefa_custos)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import requests\n",
    "import pandas as pd\n",
    "\n",
    "# --- URL completa com todos os parÃ¢metros codificados ---\n",
    "url_completa = (\n",
    "    \"https://extracao.useallcloud.com.br/api/v1/json?\"\n",
    "    \"Identificacao=m2_estoque_custos&\"\n",
    "    \"FiltrosSqlQuery=[\"\n",
    "    \"%7B%22Nome%22%3A%22idfilial%22%2C%22Valor%22%3A%5B333%2C334%2C335%2C336%2C520%2C387%2C404%2C558%2C578%2C340%2C341%2C339%2C342%2C343%2C344%2C345%2C346%2C381%2C389%2C390%5D%2C%22Operador%22%3A1%2C%22Descricao%22%3A%22Filial%22%2C%22ValorFormatado%22%3A%22SETUP%20AUTOMACAO%20E%20SEGURANCA%2C%20LOJA%20-%20ARARANGUA%2C%20LOJA%20-%20CRICIUMA%2C%20SETUP%20ELDORADO%20DO%20SUL%2C%20SETUP%20BAHIA%2C%20SETUP%20PELOTAS%2C%20SETUP%20BRASILIA%2C%20SETUP%20OS%C3%93RIO%2C%20VIGILANCIA%20SETUP%2C%20SETUP%20COMERCIO%20MATRIZ%2C%20PINHEIRINHO%20SERVI%C3%87OS%2C%20CTFM%20-%20ILLUMINATIO%20ARARANGUA%2C%20CTFM%20-%20ILLUMINATIO%20CD%2C%20VM%20-%20DISTRIBUIDORA%20ARARANGUA%20MATRIZ%2C%20VM%20-%20DISTRIBUIDORA%20CD%2C%20VM%20-%20DISTRIBUIDORA%20CRICIUMA%2C%20ENGECO%20PROJETOS%20E%20CONSTRUCOES%20LTDA%2C%20FFW%20ADMINISTRADORA%20DE%20BENS%2C%20SETUP%20LOCA%C3%87%C3%95ES%22%2C%22TipoPeriodoData%22%3Anull%7D%2C\"\n",
    "    \"%7B%22Nome%22%3A%22FILTROSREGISTROSATIVO%22%2C%22Valor%22%3A%22%20AND%20IA.ATIVO%20%3D%201%20AND%20I.ATIVO%20%3D%201%22%7D%2C\"\n",
    "    \"%7B%22Nome%22%3A%22filtroswhere%22%2C%22Valor%22%3A%22%20AND%20IDFILIAL%20IN%20(333%2C334%2C335%2C336%2C520%2C387%2C404%2C558%2C578%2C340%2C341%2C339%2C342%2C343%2C344%2C345%2C346%2C381%2C389%2C390)%22%7D%2C\"\n",
    "    \"%7B%22Nome%22%3A%22data%22%2C%22Valor%22%3A%2213/01/2026%22%7D]\"\n",
    ")\n",
    "\n",
    "# --- Headers ---\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"use-relatorio-token\": \"eyJJZCI6ImNkMDNkMzhiLWZhNzUtNDg4Yi04NDA1LTg5OTU1MzBjNjFiMSIsIlN0cmluZ0NvbmV4YW8iOiJBeHh4IFh2a2VqST10bGRxZGZ4Rzk7c3FJMSBCbz1ScUl4WHlnYWVoO2dNcXp6djFvPXNxSXVleGd5SW5TSSQkOyIsIkNvZGlnb1VzdWFyaW8iOjczMzIsIkNvZGlnb1RlbmFudCI6MTQzfQ==\"  # substitua pelo seu token real\n",
    "}\n",
    "\n",
    "# --- RequisiÃ§Ã£o direta ---\n",
    "response = requests.get(url_completa, headers=headers, timeout=3000)\n",
    "\n",
    "# --- Converter JSON para DataFrame ---\n",
    "df = pd.DataFrame(response.json())\n",
    "print(df.head())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carregar_tarefa_complexa(tarefa_atendimento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando Tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. VerificaÃ§Ã£o de Tipos ---\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] --- INICIANDO VERIFICAÃ‡ÃƒO DE TIPOS ---\")\n",
    "verificar_tipos_dados()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConfiguraÃ§Ãµes Banco de Dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "\n",
    "DB_URL = \"postgresql+psycopg2://postgres:4102@localhost:5432/SETUP\"\n",
    "\n",
    "PG_CONN_INFO = {\n",
    "    \"dbname\": \"SETUP\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"4102\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "}\n",
    "\n",
    "SCHEMA = \"useall\"\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# garante schema\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\"))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging - Bronze - Dados Brutos tipos indefinidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_staging = [\n",
    "    # simples iniciais\n",
    "    \"dfuseallitens\",\n",
    "    \"dfuseallunidades\",\n",
    "    \"dfuseallsegmentos\",\n",
    "    \"dfuseallcidades\",\n",
    "\n",
    "    # complexas no meio\n",
    "    \"dfuseallrequisicoes\",\n",
    "    \"dfuseallestoque\",\n",
    "    \"dfuseallatendimentodereq\",\n",
    "\n",
    "    # simples finais\n",
    "    \"dfuseallsolcompra\",\n",
    "    \"dfuseallfiliais\",\n",
    "    \"dfuseallempresas\",\n",
    "    \"dfuseallexpediÃ§Ã£o\",\n",
    "    \"dfuseallclientesfornecedore\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "\n",
    "def copy_df_to_postgres(df, schema: str, table: str):\n",
    "    import psycopg2\n",
    "    import io\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(\n",
    "        buffer,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        sep=\"\\t\",\n",
    "        na_rep=\"\\\\N\"\n",
    "    )\n",
    "    buffer.seek(0)\n",
    "\n",
    "    conn = psycopg2.connect(**PG_CONN_INFO)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        COPY {schema}.{table}\n",
    "        FROM STDIN\n",
    "        WITH (FORMAT CSV, DELIMITER E'\\t', NULL '\\\\N')\n",
    "    \"\"\"\n",
    "\n",
    "    cur.copy_expert(sql, buffer)\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "tabelas_criadas = 0\n",
    "dfs_nao_encontrados = []\n",
    "\n",
    "log(\"INICIANDO CARGA STAGING (COPY FROM)\")\n",
    "\n",
    "for df_nome in ordem_staging:\n",
    "    df = globals().get(df_nome)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        dfs_nao_encontrados.append(df_nome)\n",
    "        continue\n",
    "\n",
    "    tabela = \"staging_\" + df_nome.replace(\"dfuseall\", \"\")\n",
    "    tabela = tabela.lower()\n",
    "\n",
    "    log(f\"Preparando tabela {SCHEMA}.{tabela} | Linhas: {len(df)}\")\n",
    "\n",
    "    # 1ï¸âƒ£ cria estrutura (DDL leve)\n",
    "    with engine.connect() as conn:\n",
    "        df.head(0).to_sql(\n",
    "            name=tabela,\n",
    "            con=conn,\n",
    "            schema=SCHEMA,\n",
    "            if_exists=\"replace\",\n",
    "            index=False\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "    log(f\"Iniciando COPY para {SCHEMA}.{tabela}\")\n",
    "\n",
    "    # 2ï¸âƒ£ carga pesada via COPY\n",
    "    copy_df_to_postgres(df, SCHEMA, tabela)\n",
    "\n",
    "    log(f\"[OK] Tabela {SCHEMA}.{tabela} carregada com sucesso\")\n",
    "\n",
    "    tabelas_criadas += 1\n",
    "\n",
    "\n",
    "# ---------------- FINAL ----------------\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "\n",
    "if tabelas_criadas == 0:\n",
    "    log(\"Nenhuma tabela staging foi criada.\")\n",
    "    log(\"DataFrames nÃ£o encontrados:\")\n",
    "    for nome in dfs_nao_encontrados:\n",
    "        log(f\" - {nome}\")\n",
    "else:\n",
    "    log(f\"{tabelas_criadas} tabelas staging criadas com sucesso.\")\n",
    "\n",
    "log(\"PROCESSO FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver definindo tipos automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = \"useall\"\n",
    "SAMPLE_LIMIT = 50000\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Detecta formato de data\n",
    "def is_date_series(s: pd.Series):\n",
    "    sample = s.dropna().astype(str).head(50)\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d/%m/%Y %H:%M:%S\",\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            pd.to_datetime(sample, format=fmt)\n",
    "            return fmt\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# InferÃªncia de tipo\n",
    "def infer_column_type_final(series: pd.Series) -> dict:\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return {\"type\": \"text\"}\n",
    "\n",
    "    # BOOLEAN lÃ³gico\n",
    "    if s.astype(str).isin([\"0\",\"1\",\"true\",\"false\",\"True\",\"False\"]).all():\n",
    "        return {\"type\": \"boolean\"}\n",
    "\n",
    "    # DATE / TIMESTAMP\n",
    "    date_fmt = is_date_series(s)\n",
    "    if date_fmt:\n",
    "        return {\"type\": \"timestamp\", \"format\": date_fmt}\n",
    "\n",
    "    # INTEGER\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+\").all():\n",
    "        return {\"type\": \"bigint\"}\n",
    "\n",
    "    # DECIMAL\n",
    "    if s.astype(str).str.fullmatch(r\"-?\\d+(\\.\\d+)?\").all():\n",
    "        return {\"type\": \"numeric(18,4)\"}\n",
    "\n",
    "    return {\"type\": \"text\"}\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = :schema\n",
    "          AND table_type = 'BASE TABLE'\n",
    "          AND table_name LIKE 'staging_%'\n",
    "    \"\"\"), {\"schema\": SCHEMA})\n",
    "\n",
    "    staging_tables = [row[0] for row in result.fetchall()]\n",
    "\n",
    "# FunÃ§Ã£o nome silver\n",
    "def silver_table_name(staging_table: str) -> str:\n",
    "    return staging_table.replace(\"staging_\", \"silver_\")\n",
    "\n",
    "# Monta dicionÃ¡rio de metadata\n",
    "schema_silver = {}\n",
    "\n",
    "for staging_table in staging_tables:\n",
    "    if not staging_table.startswith(\"staging_\"):\n",
    "        continue\n",
    "    silver_table = silver_table_name(staging_table)\n",
    "    log(f\"Profiling {SCHEMA}.{staging_table} -> {SCHEMA}.{silver_table}\")\n",
    "\n",
    "    df_sample = pd.read_sql(\n",
    "    f'SELECT * FROM {SCHEMA}.\"{staging_table}\" LIMIT {SAMPLE_LIMIT}',\n",
    "    engine\n",
    "    )\n",
    "\n",
    "    schema_silver[silver_table] = {\n",
    "        \"staging_table\": staging_table,\n",
    "        \"columns\": {\n",
    "            col.lower(): {\n",
    "                **infer_column_type_final(df_sample[col]),\n",
    "                \"source_col\": col  # nome REAL na staging\n",
    "            }\n",
    "            for col in df_sample.columns\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Cria cast SQL\n",
    "def generate_cast_sql(col_dest, meta):\n",
    "    col_src = meta[\"source_col\"]\n",
    "\n",
    "    col_sql = f'\"{col_src}\"'\n",
    "    col_txt = f'{col_sql}::text'\n",
    "\n",
    "    if meta[\"type\"] == \"boolean\":\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN lower({col_txt}) IN ('1','true','sim','s','y','yes') THEN true\n",
    "            WHEN lower({col_txt}) IN ('0','false','nao','n','no') THEN false\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    if meta[\"type\"] == \"timestamp\":\n",
    "        fmt = meta.get(\"format\")\n",
    "        if fmt:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_txt} = '' THEN NULL\n",
    "                ELSE {col_txt}::{ 'timestamp' if 'H' in fmt else 'date' }\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return f\"\"\"\n",
    "            CASE\n",
    "                WHEN {col_sql} IS NULL OR {col_txt} = '' THEN NULL\n",
    "                ELSE {col_sql}::timestamp\n",
    "            END AS \"{col_dest}\"\n",
    "            \"\"\"\n",
    "\n",
    "    if meta[\"type\"] in (\"bigint\",\"numeric(18,4)\"):\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "            WHEN {col_txt} ~ '^-?\\\\d+(\\\\.\\\\d+)?$' THEN {col_txt}::{meta[\"type\"]}\n",
    "            ELSE NULL\n",
    "        END AS \"{col_dest}\"\n",
    "        \"\"\"\n",
    "\n",
    "    return f'{col_sql}::text AS \"{col_dest}\"'\n",
    "\n",
    "# Gera CREATE TABLE\n",
    "def generate_create_table(schema, table, columns: dict):\n",
    "    cols = \",\\n  \".join(f'\"{col_dest}\" {meta[\"type\"]}' for col_dest, meta in columns.items())\n",
    "    return f\"\"\"\n",
    "    DROP TABLE IF EXISTS {schema}.\"{table}\";\n",
    "    CREATE TABLE {schema}.\"{table}\" (\n",
    "      {cols}\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "# Cria tabelas silver\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    log(f\"Criando tabela silver {SCHEMA}.{silver_table}\")\n",
    "    ddl = generate_create_table(SCHEMA, silver_table, meta[\"columns\"])\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(ddl))\n",
    "\n",
    "# Gera INSERT\n",
    "def generate_insert_cast(staging_schema, SCHEMA, staging_table, silver_table, columns):\n",
    "    selects = \",\\n\".join(generate_cast_sql(col_dest, meta) for col_dest, meta in columns.items())\n",
    "    return f\"\"\"\n",
    "    INSERT INTO {SCHEMA}.\"{silver_table}\"\n",
    "    SELECT\n",
    "      {selects}\n",
    "    FROM {staging_schema}.\"{staging_table}\";\n",
    "    \"\"\"\n",
    "\n",
    "# Carrega dados\n",
    "for silver_table, meta in schema_silver.items():\n",
    "    staging_table = meta[\"staging_table\"]\n",
    "    columns = meta[\"columns\"]\n",
    "    log(f\"Carregando dados em {SCHEMA}.{silver_table}\")\n",
    "    sql = generate_insert_cast(SCHEMA, SCHEMA, staging_table, silver_table, columns)\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        log(f\"[OK] {SCHEMA}.{silver_table} carregada\")\n",
    "    except Exception as e:\n",
    "        log(f\"[ERRO] {SCHEMA}.{silver_table} -> {e}\")\n",
    "\n",
    "log(\"--------------------------------------------------\")\n",
    "log(\"PROCESSO FINALIZADO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold - Adicionando novas colunas e agregando valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "sql = \"\"\"\n",
    "DO $$\n",
    "DECLARE\n",
    "    r RECORD;\n",
    "    gold_table TEXT;\n",
    "BEGIN\n",
    "    FOR r IN\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'useall'\n",
    "          AND table_name LIKE 'silver_%'\n",
    "    LOOP\n",
    "\n",
    "        gold_table := replace(r.table_name, 'silver_', 'gold_');\n",
    "\n",
    "        -- CASO ESPECIAL: SILVER_REQUISICOES\n",
    "        IF r.table_name = 'silver_requisicoes' THEN\n",
    "\n",
    "            -- cria se nÃ£o existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes\n",
    "                 WHERE false;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT\n",
    "                     *,\n",
    "                     CASE status::int\n",
    "                         WHEN 0  THEN ''Digitado''\n",
    "                         WHEN 1  THEN ''Aberto''\n",
    "                         WHEN 3  THEN ''Cancelado''\n",
    "                         WHEN 10 THEN ''Parcial''\n",
    "                         WHEN 11 THEN ''Atendido''\n",
    "                         ELSE ''Desconhecido''\n",
    "                     END AS py_desc_status\n",
    "                 FROM useall.silver_requisicoes;',\n",
    "                gold_table\n",
    "            );\n",
    "\n",
    "        -- DEMAIS TABELAS\n",
    "        ELSE\n",
    "\n",
    "            -- cria se nÃ£o existir\n",
    "            EXECUTE format(\n",
    "                'CREATE TABLE IF NOT EXISTS useall.%I AS\n",
    "                 SELECT * FROM useall.%I WHERE false;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "            -- limpa e reinsere\n",
    "            EXECUTE format('TRUNCATE TABLE useall.%I;', gold_table);\n",
    "\n",
    "            EXECUTE format(\n",
    "                'INSERT INTO useall.%I\n",
    "                 SELECT * FROM useall.%I;',\n",
    "                gold_table,\n",
    "                r.table_name\n",
    "            );\n",
    "\n",
    "        END IF;\n",
    "\n",
    "    END LOOP;\n",
    "END $$;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(sql))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim_Calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ---------------- SQL ----------------\n",
    "sql_create_dim_calendario = text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS useall.dim_calendario (\n",
    "    data DATE PRIMARY KEY,\n",
    "\n",
    "    ano INT,\n",
    "    mes INT,\n",
    "    dia INT,\n",
    "\n",
    "    mes_ano TEXT,\n",
    "    mes_ano_ordem INT,\n",
    "\n",
    "    nome_mes TEXT,\n",
    "    nome_mes_abrev TEXT,\n",
    "    nome_dia TEXT,\n",
    "    nome_dia_abrev TEXT,\n",
    "\n",
    "    dia_semana INT,\n",
    "    semana_iso INT,\n",
    "    ano_iso INT,\n",
    "    trimestre INT,\n",
    "\n",
    "    is_fim_de_semana BOOLEAN,\n",
    "    is_feriado BOOLEAN,\n",
    "    nome_feriado TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "sql_create_indices = text(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_data\n",
    "    ON useall.dim_calendario (data);\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_dim_calendario_mes_ano_ordem\n",
    "    ON useall.dim_calendario (mes_ano_ordem);\n",
    "\"\"\")\n",
    "\n",
    "sql_atualiza_calendario = text(\"\"\"\n",
    "INSERT INTO useall.dim_calendario\n",
    "SELECT DISTINCT\n",
    "    d::date AS data,\n",
    "\n",
    "    EXTRACT(YEAR FROM d)::int AS ano,\n",
    "    EXTRACT(MONTH FROM d)::int AS mes,\n",
    "    EXTRACT(DAY FROM d)::int AS dia,\n",
    "\n",
    "    TO_CHAR(d, 'MM/YYYY') AS mes_ano,\n",
    "    (EXTRACT(YEAR FROM d) * 100 + EXTRACT(MONTH FROM d))::int AS mes_ano_ordem,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Janeiro'\n",
    "        WHEN 2 THEN 'Fevereiro'\n",
    "        WHEN 3 THEN 'MarÃ§o'\n",
    "        WHEN 4 THEN 'Abril'\n",
    "        WHEN 5 THEN 'Maio'\n",
    "        WHEN 6 THEN 'Junho'\n",
    "        WHEN 7 THEN 'Julho'\n",
    "        WHEN 8 THEN 'Agosto'\n",
    "        WHEN 9 THEN 'Setembro'\n",
    "        WHEN 10 THEN 'Outubro'\n",
    "        WHEN 11 THEN 'Novembro'\n",
    "        WHEN 12 THEN 'Dezembro'\n",
    "    END AS nome_mes,\n",
    "\n",
    "    CASE EXTRACT(MONTH FROM d)\n",
    "        WHEN 1 THEN 'Jan'\n",
    "        WHEN 2 THEN 'Fev'\n",
    "        WHEN 3 THEN 'Mar'\n",
    "        WHEN 4 THEN 'Abr'\n",
    "        WHEN 5 THEN 'Mai'\n",
    "        WHEN 6 THEN 'Jun'\n",
    "        WHEN 7 THEN 'Jul'\n",
    "        WHEN 8 THEN 'Ago'\n",
    "        WHEN 9 THEN 'Set'\n",
    "        WHEN 10 THEN 'Out'\n",
    "        WHEN 11 THEN 'Nov'\n",
    "        WHEN 12 THEN 'Dez'\n",
    "    END AS nome_mes_abrev,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Segunda-feira'\n",
    "        WHEN 2 THEN 'TerÃ§a-feira'\n",
    "        WHEN 3 THEN 'Quarta-feira'\n",
    "        WHEN 4 THEN 'Quinta-feira'\n",
    "        WHEN 5 THEN 'Sexta-feira'\n",
    "        WHEN 6 THEN 'SÃ¡bado'\n",
    "        WHEN 7 THEN 'Domingo'\n",
    "    END AS nome_dia,\n",
    "\n",
    "    CASE EXTRACT(ISODOW FROM d)\n",
    "        WHEN 1 THEN 'Seg'\n",
    "        WHEN 2 THEN 'Ter'\n",
    "        WHEN 3 THEN 'Qua'\n",
    "        WHEN 4 THEN 'Qui'\n",
    "        WHEN 5 THEN 'Sex'\n",
    "        WHEN 6 THEN 'SÃ¡b'\n",
    "        WHEN 7 THEN 'Dom'\n",
    "    END AS nome_dia_abrev,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d)::int AS dia_semana,\n",
    "    EXTRACT(WEEK FROM d)::int AS semana_iso,\n",
    "    EXTRACT(ISOYEAR FROM d)::int AS ano_iso,\n",
    "    EXTRACT(QUARTER FROM d)::int AS trimestre,\n",
    "\n",
    "    EXTRACT(ISODOW FROM d) IN (6,7) AS is_fim_de_semana,\n",
    "    FALSE AS is_feriado,\n",
    "    NULL AS nome_feriado\n",
    "FROM (\n",
    "    SELECT DISTINCT data::date AS d\n",
    "    FROM useall.gold_requisicoes\n",
    "    WHERE data IS NOT NULL\n",
    ") x\n",
    "ON CONFLICT (data) DO NOTHING;\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- EXECUÃ‡ÃƒO ----------------\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql_create_dim_calendario)\n",
    "    conn.execute(sql_create_indices)\n",
    "    conn.execute(sql_atualiza_calendario)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo Tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from collections import defaultdict\n",
    "import os # Importado para manipulaÃ§Ã£o de caminhos/diretÃ³rios\n",
    "# from IPython.display import display # Descomente se estiver usando Jupyter Notebook/Lab\n",
    "\n",
    "# ================= CONFIG =================\n",
    "DB_URL = \"postgresql+psycopg2://postgres:4102@localhost:5432/SETUP\"\n",
    "SCHEMA = \"useall\"\n",
    "FK_MATCH_MIN = 0.8       # mÃ­nimo de correspondÃªncia para considerar FK\n",
    "SAMPLE_LIMIT = 1000      # limitar linhas para tabelas grandes\n",
    "EXCEL_OUTPUT_PATH = r\"..\\notebooks\\tests\\relacionamentos_sugeridos.xlsx\"\n",
    "GRAPH_OUTPUT_DIR = r\"..\\notebooks\\tests\" # Use r\"\" para caminhos do Windows\n",
    "GRAPH_OUTPUT_FILENAME = \"grafo_relacionamentos.png\"\n",
    "\n",
    "# ================= CONEXÃƒO =================\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# (O restante do seu cÃ³digo de Metadados, Filtro Gold, PKs Candidatas e FKs Candidatas permanece o mesmo...)\n",
    "\n",
    "# ================= METADADOS =================\n",
    "cols = pd.read_sql(f\"\"\"\n",
    "SELECT table_name, column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = '{SCHEMA}'\n",
    "\"\"\", engine)\n",
    "\n",
    "# ================= FILTRO GOLD =================\n",
    "gold_tables = [t for t in cols.table_name.unique() if t.startswith(\"gold_\") or t == \"dim_calendario\"]\n",
    "cols = cols[cols.table_name.isin(gold_tables)]\n",
    "tables = cols.table_name.unique()\n",
    "\n",
    "# ================= PKs CANDIDATAS =================\n",
    "pk_candidatas = {}\n",
    "for table in tables:\n",
    "    table_cols = cols[cols.table_name == table].column_name\n",
    "    for col in table_cols:\n",
    "        q = f\"\"\"\n",
    "        SELECT COUNT(DISTINCT \"{col}\") = COUNT(\"{col}\") AS is_pk\n",
    "        FROM {SCHEMA}.\"{table}\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ok = pd.read_sql(q, engine).iloc\n",
    "            if ok:\n",
    "                pk_candidatas.setdefault(table, []).append(col)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# ================= PK DF COM DATA_TYPE =================\n",
    "pk_df = pd.merge(\n",
    "    pd.DataFrame([\n",
    "        {\"table_name\": t, \"column_name\": c}\n",
    "        for t, cols_ in pk_candidatas.items()\n",
    "        for c in cols_\n",
    "    ]),\n",
    "    cols[['table_name', 'column_name', 'data_type']],\n",
    "    on=['table_name', 'column_name'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ================= FK CANDIDATAS =================\n",
    "potential_fk_links = []\n",
    "for _, c_origem in cols.iterrows():\n",
    "    for _, c_dest in pk_df.iterrows():\n",
    "        if c_origem.table_name == c_dest.table_name:\n",
    "            continue\n",
    "        if c_origem.data_type != c_dest.data_type:\n",
    "            continue\n",
    "        potential_fk_links.append({\n",
    "            \"origem_tbl\": c_origem.table_name,\n",
    "            \"origem_col\": c_origem.column_name,\n",
    "            \"dest_tbl\": c_dest.table_name,\n",
    "            \"dest_col\": c_dest.column_name,\n",
    "            \"data_type\": c_origem.data_type\n",
    "        })\n",
    "\n",
    "\n",
    "# ================= ANÃLISE DE CARDINALIDADE (AJUSTADO) =================\n",
    "\n",
    "final_relationships = []\n",
    "sampled_data = {}\n",
    "# Conjunto para rastrear relacionamentos jÃ¡ vistos, independentemente da direÃ§Ã£o\n",
    "seen_relationships = set() \n",
    "\n",
    "def get_sampled_data(table_name, schema, limit, engine):\n",
    "    if table_name not in sampled_data:\n",
    "        q = f'SELECT * FROM {schema}.\"{table_name}\" LIMIT {limit}'\n",
    "        sampled_data[table_name] = pd.read_sql(q, engine)\n",
    "    return sampled_data[table_name]\n",
    "\n",
    "for link in potential_fk_links:\n",
    "    tbl_o = link['origem_tbl']\n",
    "    col_o = link['origem_col']\n",
    "    tbl_d = link['dest_tbl']\n",
    "    col_d = link['dest_col']\n",
    "\n",
    "    # --- LÃ³gica de NormalizaÃ§Ã£o/Rastreamento ---\n",
    "    # Crie uma chave Ãºnica para o par, ordenando as tabelas e colunas alfabeticamente\n",
    "    # para que (A, B) e (B, A) resultem na mesma chave rastreÃ¡vel.\n",
    "    rel_key = tuple(sorted(((tbl_o, col_o), (tbl_d, col_d))))\n",
    "    \n",
    "    if rel_key in seen_relationships:\n",
    "        continue # Pula este link se o par reverso/duplicado jÃ¡ foi processado\n",
    "\n",
    "    try:\n",
    "        df_o = get_sampled_data(tbl_o, SCHEMA, SAMPLE_LIMIT, engine)\n",
    "        df_d = get_sampled_data(tbl_d, SCHEMA, SAMPLE_LIMIT, engine)\n",
    "\n",
    "        match_count = df_o[col_o].isin(df_d[col_d]).sum()\n",
    "        total_count_o = len(df_o)\n",
    "        match_perc = match_count / total_count_o if total_count_o > 0 else 0\n",
    "\n",
    "        if match_perc < FK_MATCH_MIN:\n",
    "            continue\n",
    "\n",
    "        is_origin_col_unique = df_o[col_o].nunique() >= total_count_o * 0.95\n",
    "        \n",
    "        rel_type = \"Muitos para Muitos (N:N)\"\n",
    "        if not is_origin_col_unique:\n",
    "             rel_type = \"Muitos para Um (N:1)\"\n",
    "        elif is_origin_col_unique and df_d[col_d].nunique() == len(df_d):\n",
    "             rel_type = \"Um para Um (1:1)\"\n",
    "\n",
    "        final_relationships.append({\n",
    "            \"tabela_origem\": tbl_o,\n",
    "            \"coluna_origem\": col_o,\n",
    "            \"tabela_destino\": tbl_d,\n",
    "            \"coluna_destino\": col_d,\n",
    "            \"tipo_relacionamento\": rel_type,\n",
    "            \"porcentagem_match\": round(match_perc * 100, 2)\n",
    "        })\n",
    "        \n",
    "        # Marca este relacionamento como visto\n",
    "        seen_relationships.add(rel_key)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao analisar cardinalidade {tbl_o}.{col_o} <-> {tbl_d}.{col_d}: {e}\")\n",
    "\n",
    "# ================= DISPLAY E EXPORTAÃ‡ÃƒO (NOVA SEÃ‡ÃƒO) =================\n",
    "if final_relationships:\n",
    "    rel_df = pd.DataFrame(final_relationships)\n",
    "    \n",
    "    print(\"\\n--- Relacionamentos Sugeridos e Porcentagens de Match ---\")\n",
    "    # Para Notebooks (Jupyter/Colab), use display(rel_df) para um output HTML bonito:\n",
    "    display(rel_df)\n",
    "\n",
    "    # Exportar para Excel\n",
    "    try:\n",
    "        rel_df.to_excel(EXCEL_OUTPUT_PATH, index=False, engine='openpyxl') #\n",
    "        print(f\"\\nâœ… Tabela exportada com sucesso para '{EXCEL_OUTPUT_PATH}'\")\n",
    "    except ImportError:\n",
    "        print(\"\\nâŒ NÃ£o foi possÃ­vel exportar para Excel. Instale 'openpyxl' (pip install openpyxl) ou 'xlsxwriter'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Erro ao exportar para Excel: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNenhum relacionamento forte encontrado com base nos critÃ©rios definidos.\")\n",
    "\n",
    "\n",
    "# ================= GRAFO (SeÃ§Ã£o Original Atualizada) =================\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for fk in final_relationships:\n",
    "    if fk[\"porcentagem_match\"] == 100:\n",
    "        label_text = f'{fk[\"coluna_origem\"]} â†’ {fk[\"coluna_destino\"]} \\n({fk[\"tipo_relacionamento\"]} | {fk[\"porcentagem_match\"]}%)'\n",
    "        G.add_edge(\n",
    "            fk[\"tabela_origem\"],\n",
    "            fk[\"tabela_destino\"],\n",
    "            label=label_text\n",
    "        )\n",
    "\n",
    "# ================= VISUAL E SALVAR IMAGEM (NOVA SEÃ‡ÃƒO) =================\n",
    "plt.figure(figsize=(18, 14))\n",
    "pos = nx.spring_layout(G, k=0.8)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2200, node_color=\"#007b5e\", alpha=0.9)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_color=\"white\")\n",
    "nx.draw_networkx_edges(G, pos, arrowstyle='-|>', arrowsize=20, edge_color=\"#004d3b\")\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "plt.title(\"RelaÃ§Ãµes PK â†’ FK Candidatas e Tipo de Relacionamento (Gold)\", fontsize=16)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Salvar o grÃ¡fico no caminho especificado antes de mostrar\n",
    "# Garante que o diretÃ³rio exista\n",
    "os.makedirs(GRAPH_OUTPUT_DIR, exist_ok=True)\n",
    "full_output_path = os.path.join(GRAPH_OUTPUT_DIR, GRAPH_OUTPUT_FILENAME)\n",
    "\n",
    "try:\n",
    "    # Use bbox_inches='tight' para garantir que os rÃ³tulos nÃ£o sejam cortados\n",
    "    plt.savefig(full_output_path, bbox_inches='tight', dpi=100) \n",
    "    print(f\"\\nðŸ–¼ï¸ GrÃ¡fico salvo com sucesso em '{full_output_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Erro ao salvar o grÃ¡fico: {e}\")\n",
    "\n",
    "# Mostrar o grÃ¡fico (se estiver em um ambiente interativo como Jupyter ou rodando localmente)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "# Filtra o DataFrame deduplicado e usa a porcentagem para ordenar\n",
    "df_sorted = rel_df.sort_values(by=\"porcentagem_match\", ascending=True)\n",
    "\n",
    "# Cria rÃ³tulos combinados para o eixo Y\n",
    "labels = df_sorted[\"tabela_origem\"] + \" -> \" + df_sorted[\"tabela_destino\"]\n",
    "\n",
    "sns.barplot(x=\"porcentagem_match\", y=labels, data=df_sorted)\n",
    "plt.title(\"Porcentagem de Match por Relacionamento (Ordenado)\")\n",
    "plt.xlabel(\"Porcentagem de Match (%)\")\n",
    "plt.ylabel(\"Relacionamento\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pivotar os dados para criar a matriz Tabela Origem x Tabela Destino\n",
    "heatmap_data = rel_df.pivot_table(\n",
    "    index=\"tabela_origem\", \n",
    "    columns=\"tabela_destino\", \n",
    "    values=\"porcentagem_match\", \n",
    "    aggfunc='mean' # Se houver mÃºltiplas colunas entre as mesmas tabelas\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title(\"Heatmap de Porcentagem de Match entre Tabelas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "import itertools # Importado para gerar combinaÃ§Ãµes de colunas\n",
    "import os\n",
    "\n",
    "# ================= CONFIG =================\n",
    "DB_URL = \"postgresql+psycopg2://postgres:4102@localhost:5432/SETUP\"\n",
    "SCHEMA = \"useall\"\n",
    "SAMPLE_LIMIT = 1000      # limitar linhas para tabelas grandes\n",
    "GRAPH_OUTPUT_DIR = r\"tests\"\n",
    "GRAPH_OUTPUT_FILENAME_HEATMAP = \"heatmap_similaridade_colunas.png\"\n",
    "\n",
    "# ================= CONEXÃƒO =================\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# ================= METADADOS =================\n",
    "cols = pd.read_sql(f\"\"\"\n",
    "SELECT table_name, column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = '{SCHEMA}'\n",
    "\"\"\", engine)\n",
    "\n",
    "# ================= FILTRO GOLD =================\n",
    "gold_tables = [t for t in cols.table_name.unique() if t.startswith(\"gold_\") or t == \"dim_calendario\"]\n",
    "cols = cols[cols.table_name.isin(gold_tables)]\n",
    "tables = cols.table_name.unique()\n",
    "\n",
    "# ================= FUNÃ‡ÃƒO DE SIMILARIDADE =================\n",
    "\n",
    "sampled_data_cache = {}\n",
    "\n",
    "def get_sampled_data(table_name, schema, limit, engine):\n",
    "    \"\"\"Carrega dados sampleados em cache.\"\"\"\n",
    "    if table_name not in sampled_data_cache:\n",
    "        q = f'SELECT \"{col_name}\" FROM {schema}.\"{table_name}\" LIMIT {limit}'\n",
    "        sampled_data_cache[table_name] = pd.read_sql(q, engine)\n",
    "    return sampled_data_cache[table_name]\n",
    "\n",
    "def calculate_jaccard_similarity(col_a_meta, col_b_meta, schema, limit, engine):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade de Jaccard entre duas colunas: |InterseÃ§Ã£o| / |UniÃ£o|.\n",
    "    Retorna um valor entre 0.0 e 1.0.\n",
    "    \"\"\"\n",
    "    tbl_a, col_a = col_a_meta['table_name'], col_a_meta['column_name']\n",
    "    tbl_b, col_b = col_b_meta['table_name'], col_b_meta['column_name']\n",
    "\n",
    "    # Se a(s) tabela(s) nÃ£o puder(em) ser amostrada(s) (e.g., erro de SQL), retorne 0\n",
    "    try:\n",
    "        df_a = get_sampled_data(tbl_a, schema, limit, engine)\n",
    "        df_b = get_sampled_data(tbl_b, schema, limit, engine)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    # Pega os conjuntos de valores Ãºnicos (e remove NaNs para o cÃ¡lculo)\n",
    "    set_a = set(df_a[col_a].dropna())\n",
    "    set_b = set(df_b[col_b].dropna())\n",
    "    \n",
    "    if not set_a and not set_b:\n",
    "        return 1.0 # Ambos vazios, similaridade mÃ¡xima\n",
    "\n",
    "    intersection_size = len(set_a.intersection(set_b))\n",
    "    union_size = len(set_a.union(set_b))\n",
    "    \n",
    "    if union_size == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return intersection_size / union_size\n",
    "\n",
    "# ================= CALCULAR TODAS AS SIMILARIDADES =================\n",
    "\n",
    "print(\"\\n--- Tipos de dados Ãºnicos encontrados no seu schema 'useall' ---\")\n",
    "print(cols['data_type'].unique())\n",
    "print(\"-------------------------------------------------------------------\\n\")\n",
    "\n",
    "tipos_compatÃ­veis = ['integer', 'bigint', 'numeric', 'text', 'timestamp without time zone', 'boolean']\n",
    "comparable_cols = cols[cols['data_type'].isin(tipos_compatÃ­veis)]\n",
    "\n",
    "column_list = comparable_cols.to_dict('records')\n",
    "similarity_data = []\n",
    "\n",
    "print(f\"\\nIniciando cÃ¡lculo de similaridade para {len(column_list)} colunas. Isso pode demorar...\")\n",
    "\n",
    "# Usamos itertools.combinations para evitar calcular A->B e B->A separadamente e A->A\n",
    "for col_a_meta, col_b_meta in itertools.combinations(column_list, 2):\n",
    "    # SÃ³ compara se os tipos de dados bÃ¡sicos sÃ£o iguais\n",
    "    if col_a_meta['data_type'] == col_b_meta['data_type']:\n",
    "        sim = calculate_jaccard_similarity(col_a_meta, col_b_meta, SCHEMA, SAMPLE_LIMIT, engine)\n",
    "        if sim > 0: # Adiciona apenas similaridades maiores que zero para performance/clareza\n",
    "             similarity_data.append({\n",
    "                'col_a': f\"{col_a_meta['table_name']}.{col_a_meta['column_name']}\",\n",
    "                'col_b': f\"{col_b_meta['table_name']}.{col_b_meta['column_name']}\",\n",
    "                'similarity': sim * 100 # Em porcentagem\n",
    "            })\n",
    "\n",
    "# Converte para DataFrame para fÃ¡cil manipulaÃ§Ã£o\n",
    "similarity_df = pd.DataFrame(similarity_data)\n",
    "\n",
    "# ================= GERAR HEATMAP =================\n",
    "\n",
    "if not similarity_df.empty:\n",
    "    # Pivotar para o formato de matriz necessÃ¡rio para o heatmap\n",
    "    heatmap_matrix = similarity_df.pivot_table(\n",
    "        index='col_a', \n",
    "        columns='col_b', \n",
    "        values='similarity', \n",
    "        fill_value=0 # Preenche pares nÃ£o comparados com 0%\n",
    "    )\n",
    "    \n",
    "    # Adicionar 100% na diagonal (similaridade de uma coluna consigo mesma)\n",
    "    for col_name in heatmap_matrix.columns:\n",
    "        if col_name in heatmap_matrix.index:\n",
    "            heatmap_matrix.loc[col_name, col_name] = 100.0\n",
    "\n",
    "    print(\"\\nGerando visualizaÃ§Ã£o do Heatmap...\")\n",
    "    plt.figure(figsize=(20, 18)) # Aumente o tamanho para caber muitos rÃ³tulos\n",
    "    sns.heatmap(heatmap_matrix, annot=False, fmt=\".1f\", cmap=\"YlGnBu\", linewidths=.1, square=True)\n",
    "    plt.title(\"Heatmap de Similaridade (Jaccard) entre Todas as Colunas CompatÃ­veis (%)\", fontsize=16)\n",
    "    plt.xlabel(\"Colunas de Destino\")\n",
    "    plt.ylabel(\"Colunas de Origem\")\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    \n",
    "    # Salvar o grÃ¡fico no caminho especificado\n",
    "    os.makedirs(GRAPH_OUTPUT_DIR, exist_ok=True)\n",
    "    full_output_path = os.path.join(GRAPH_OUTPUT_DIR, GRAPH_OUTPUT_FILENAME_HEATMAP)\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(full_output_path, bbox_inches='tight', dpi=100) \n",
    "        print(f\"\\nðŸ–¼ï¸ Heatmap salvo com sucesso em '{full_output_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Erro ao salvar o heatmap: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nNenhuma similaridade encontrada entre as colunas selecionadas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
